{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390bcda0-5ad4-4dc1-a274-4f066a26d0f0",
   "metadata": {},
   "source": [
    "## pip install everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e710ec58-cd53-46a1-ac7d-4a415f14ccd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !sudo yum install Xvfb -y -y\n",
    "# !pip install scrapy\n",
    "# !pip install fake_useragent\n",
    "# !pip install scrapy-selenium\n",
    "# !pip install selenium==4.9.1\n",
    "# !pip install boto3==1.28.5\n",
    "# !pip install crochet\n",
    "# !pip install awslambdaric\n",
    "# !pip install botocore==1.31.10\n",
    "# #\n",
    "\n",
    "# import os\n",
    "# cmd = '''\n",
    "# sudo yum install atk cups-libs gtk3 libXcomposite alsa-lib libXcursor libXdamage libXext libXi libXrandr libXScrnSaver libXtst pango at-spi2-atk libXt xorg-x11-server-Xvfb xorg-x11-xauth dbus-glib dbus-glib-devel -y\n",
    "# '''\n",
    "# os.system(cmd)\n",
    "\n",
    "\n",
    "# !curl  -Lo \"/tmp/chromedriver.zip\" \"https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip\"\n",
    "# !unzip /tmp/chromedriver.zip -d /opt/ -y\n",
    "# !sudo ln -sf /opt/chromedriver /usr/local/bin/chromedriver\n",
    "\n",
    "# !curl -Lo \"/tmp/chrome-linux.zip\" \"https://www.googleapis.com/download/storage/v1/b/chromium-browser-snapshots/o/Linux_x64%2F1135561%2Fchrome-linux.zip?alt=media\"\n",
    "# !unzip /tmp/chrome-linux.zip -d /opt/ -y\n",
    "# !sudo ln -sf /opt/chrome-linux/chrome /usr/local/bin/chrome\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a865491-82bb-4cf7-9e80-a11ff2d588e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf1db375-5038-41be-b82a-de1a56a5d0f3",
   "metadata": {},
   "source": [
    "### In terminal \n",
    "\n",
    "$sudo chmod 777 /etc/yum.repos.d/docker-ce.repo\n",
    "\n",
    "$vi /etc/yum.repos.d/docker-ce.repo\n",
    "\n",
    "\n",
    "#### [file]\n",
    "1. name=Docker CE Stable - $basearch\n",
    "\n",
    "    ##baseurl=https://download.docker.com/linux/centos/$releasever/$basearch/stable\n",
    "            \n",
    "2. baseurl=https://download.docker.com/linux/centos/7/$basearch/stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28bb12b3-a417-4df7-a524-fcbe4b144c78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x 1 ec2-user ec2-user 358375680 Apr 25 22:16 /opt/chrome-linux/chrome\n",
      "lrwxrwxrwx 1 root root 24 Aug 25 02:38 /usr/local/bin/chrome -> /opt/chrome-linux/chrome\n",
      "ls: cannot access /opt/chromedriver: No such file or directory\n",
      "lrwxrwxrwx 1 root root 17 Aug 25 02:38 /usr/local/bin/chromedriver -> /opt/chromedriver\n"
     ]
    }
   ],
   "source": [
    "!ls -al /opt/chrome-linux/chrome\n",
    "!ls -al /usr/local/bin/chrome\n",
    "!ls -al /opt/chromedriver \n",
    "!ls -al /usr/local/bin/chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b92a81-9248-44a7-81a0-bd69597934da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !/usr/local/bin/chrome --headless --no-sandbox --disable-gpu --remote-debugging-port=9222\n",
    "# !/usr/local/bin/chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c591d-3b2d-4cab-b8ea-92579fcddaf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## tier2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00dbc69c-bb36-49c1-92fc-2e201418631d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os, datetime, requests\n",
    "import botocore\n",
    "from fake_useragent import UserAgent\n",
    "import boto3\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from crochet import setup\n",
    "import threading\n",
    "from momoshop import MomoshopSpider\n",
    "# from momoshop_tier4 import MomoshopSpider as MomoshopSpider_tier4\n",
    "\n",
    "# Initialize Crochet\n",
    "setup()\n",
    "\n",
    "ua = UserAgent()\n",
    "region = os.getenv('ap-northeast-1')\n",
    "sqs = boto3.client('sqs', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "queue_tier2_links = os.getenv('https://sqs.ap-northeast-1.amazonaws.com/407620147666/aws_crawler_tier2_links.fifo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d58b66-e4c7-474d-bb95-d67bf42bae44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LambdaRunner:\n",
    "    category_links = []\n",
    "\n",
    "    def __init__(self):\n",
    "        self.finished = threading.Event()\n",
    "        self.results = []\n",
    "\n",
    "    def run_spider(self):\n",
    "        # Create a CrawlerRunner with project settings\n",
    "        settings = get_project_settings()\n",
    "        runner = CrawlerRunner(settings)\n",
    "\n",
    "        # Create an instance of the spider class\n",
    "        spider_cls = MomoshopSpider\n",
    "\n",
    "        # Callback function to handle the spider results\n",
    "        def handle_results(result):\n",
    "            self.results.append(result)\n",
    "\n",
    "            # Check if the spider has finished running\n",
    "            if len(self.results) == 1:\n",
    "                self.finished.set()\n",
    "\n",
    "        # Start the first spider run\n",
    "        deferred = runner.crawl(spider_cls, self)\n",
    "        deferred.addCallback(handle_results)\n",
    "\n",
    "        # Start the reactor\n",
    "        runner.join()\n",
    "\n",
    "    def wait_for_completion(self):\n",
    "        self.finished.wait()\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "    \n",
    "    \n",
    "\n",
    "def handler(event, context):\n",
    "    try:\n",
    "        print(\"Starting tier2 links crawling\")\n",
    "        runner = LambdaRunner()\n",
    "        runner.run_spider()\n",
    "        runner.wait_for_completion()\n",
    "\n",
    "        queue_list = []\n",
    "\n",
    "        # Aggregate input JSON format\n",
    "        for category_link in runner.category_links:\n",
    "            queue_list.append({'category_link': category_link})\n",
    "        print(f'len(queue_list): {len(runner.category_links)}')\n",
    "        print(\"success\")\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': {'message': queue_list}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f'fail:{str(e)}')\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({'error': str(e)})\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25713d2a-f3ef-4024-87fe-7b9ef99652ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# handler(\"\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd9a89-90fd-4d32-b75e-ab164d363a21",
   "metadata": {},
   "source": [
    "## tier 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ddd39e-6dca-4090-89da-88c3184c1bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import threading\n",
    "import scrapy\n",
    "from fake_useragent import UserAgent\n",
    "from scrapy import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "from scrapy_selenium import SeleniumRequest\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium import webdriver\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "# Initialize the UserAgent and AWS clients\n",
    "ua = UserAgent()\n",
    "region = 'ap-northeast-1'\n",
    "sqs = boto3.client('sqs', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "s3 = boto3.client('s3', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "queue_tier2_links = 'https://sqs.ap-northeast-1.amazonaws.com/407620147666/aws_crawler_tier2_links.fifo'\n",
    "queue_tier4_links = 'https://sqs.ap-northeast-1.amazonaws.com/407620147666/aws_crawler_tier4_links.fifo'\n",
    "s3_bucket_name = 's3://aws-web-crawler-data/PIC_share/'\n",
    "export_to_s3 = 'Off'\n",
    "\n",
    "\n",
    "# Function to determine the tier based on the URL\n",
    "def check_tier_by_url(url: str):\n",
    "    if url.find(\"DgrpCategory\") != -1:\n",
    "        return \"tier4\"\n",
    "    elif url.find(\"MgrpCategory\") != -1:\n",
    "        return \"tier3\"\n",
    "    elif url.find(\"LgrpCategory\") != -1:\n",
    "        return \"tier2\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to calculate SHA-256 hash of a string\n",
    "def hash_function(original_string):\n",
    "    # Create a hash object using SHA-256 algorithm\n",
    "    hasher = hashlib.sha256()\n",
    "\n",
    "    # Convert the input string to bytes (required by hashlib)\n",
    "    input_bytes = original_string.encode('utf-8')\n",
    "\n",
    "    # Update the hash object with the input bytes\n",
    "    hasher.update(input_bytes)\n",
    "\n",
    "    # Get the hexadecimal representation of the hash value\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "\n",
    "# Spider class for crawling momoshop.com.tw\n",
    "class MomoshopSpider(scrapy.Spider):\n",
    "    name = \"momoshop\"\n",
    "    allowed_domains = [\"www.momoshop.com.tw\"]\n",
    "    user_agent = ua.random\n",
    "    receipt_handle = \"\"\n",
    "    target_url = \"\"\n",
    "    runner = None\n",
    "\n",
    "    def __init__(self, url=None, runner=None, **kwargs):\n",
    "        self.target_url = url\n",
    "        self.runner = runner\n",
    "        super(MomoshopSpider, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    # Function to handle errors during parsing\n",
    "    def error_handle(self, error):\n",
    "        print(error)\n",
    "        print(f'Invalid page: {self.target_url}')\n",
    "        self.runner.timeout = True\n",
    "\n",
    "    # Entry point for the spider\n",
    "    def start_requests(self):\n",
    "        print(\"Starting queue_tier2_links from start_requests\")\n",
    "        print(f'self.target_url: {self.target_url}')\n",
    "        yield SeleniumRequest(url=self.target_url,\n",
    "                              callback=self.parse_tier2,\n",
    "                              wait_time=15,\n",
    "                              errback=self.error_handle,\n",
    "                              wait_until=ec.any_of(ec.presence_of_element_located((By.CLASS_NAME, 'first')),  # Wait for the tier path loaded\n",
    "                                                   ec.presence_of_element_located((By.ID, 'backgroundContent')),  # Wait for the landing page loaded\n",
    "                                                   ec.presence_of_element_located((By.CLASS_NAME, \"year18Ban\"))))  # Wait for the yes18years paged loaded\n",
    "\n",
    "#     # Function to send a message to SQS\n",
    "#     @staticmethod\n",
    "#     def send_to_sqs(message):\n",
    "#         try:\n",
    "#             # print(message['MessageGroupId'])\n",
    "#             response = sqs.send_message(QueueUrl=queue_tier4_links,\n",
    "#                                         MessageGroupId=message['MessageGroupId'],\n",
    "#                                         MessageBody=message['MessageBody'],\n",
    "#                                         MessageDeduplicationId=message['MessageGroupId'])\n",
    "#             if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n",
    "#                 print('Fail')\n",
    "#         except Exception as e:\n",
    "#             print(str(e))\n",
    "\n",
    "#     # Function to send data to S3\n",
    "#     @staticmethod\n",
    "#     def send_to_s3(m, hash_id):\n",
    "#         try:\n",
    "#             json_file_name = f'{hash_id}.json'\n",
    "#             response = s3.put_object(Bucket=s3_bucket_name, Key=json_file_name, Body=json.dumps(m, ensure_ascii=False))\n",
    "#             # check if it's successful\n",
    "#             if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n",
    "#                 print('Fail')\n",
    "#         except Exception as e:\n",
    "#             print(str(e))\n",
    "\n",
    "    # Function to start pushing data to SQS and S3\n",
    "    def start_to_push(self, current_tier, hash_id, tier_dict, category_link, message_body):\n",
    "        if current_tier == \"tier4\":\n",
    "            print(f'tier4: {tier_dict.get(\"tier4\", \"\")}')\n",
    "            message = {'Id': hash_id, 'MessageGroupId': hash_id,\n",
    "                       'MessageBody': json.dumps({\n",
    "                           'tier1_category_name': tier_dict[\"tier1\"],\n",
    "                           'tier2_category_name': tier_dict.get(\"tier2\", \"\"),\n",
    "                           'tier3_category_name': tier_dict.get(\"tier3\", \"\"),\n",
    "                           'tier4_category_name': tier_dict.get(\"tier4\", \"\"),\n",
    "                           'category_link': category_link if category_link.find(\n",
    "                               \"javascript\") == -1 else \"\",\n",
    "                           'category_type': current_tier}, ensure_ascii=False)}\n",
    "            print(message)\n",
    "            # self.send_to_sqs(message)\n",
    "            # self.runner.category_info.append(message_body)\n",
    "\n",
    "            # if export_to_s3 is not None and export_to_s3 == \"On\":\n",
    "                # self.send_to_s3(message_body, hash_id)\n",
    "\n",
    "    # Function to parse the tier 2 categories\n",
    "    def parse_tier2(self, response: HtmlResponse):\n",
    "        try:\n",
    "\n",
    "            print(\"Starting to parse_tier2\")\n",
    "            \n",
    "            #-------------- add to prevent bugs from jupyter-lab --------------------\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.binary_location = \"/opt/chrome-linux/chrome\"\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            options.add_argument(\"--single-process\")\n",
    "            options.add_argument(\"blink-settings=imagesEnabled=false\")\n",
    "            options.add_argument(\"--remote-debugging-port=9222\")\n",
    "            options.add_argument(\"--no-zygote\")\n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "            options.add_argument(\"--disable-dev-tools\")\n",
    "            options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "            driver = webdriver.Chrome(executable_path = '/usr/local/bin/chromedriver', chrome_options=options)\n",
    "            driver.get(response.url)\n",
    "\n",
    "            wait = WebDriverWait(driver, 20)  # Adjust the timeout as needed\n",
    "            wait.until(ec.any_of(ec.presence_of_element_located((By.CLASS_NAME, 'first')),  # Wait for the tier path loaded\n",
    "                               ec.presence_of_element_located((By.ID, 'backgroundContent')),  # Wait for the landing page loaded\n",
    "                               ec.presence_of_element_located((By.CLASS_NAME, \"year18Ban\"))))  # Wait for the yes18years paged loaded\n",
    "\n",
    "            updated_content = driver.page_source\n",
    "\n",
    "            response = scrapy.http.HtmlResponse(url=driver.current_url, \n",
    "                                                body=updated_content,\n",
    "                                                encoding='utf-8')\n",
    "\n",
    "            #----------------------------------------------------------------\n",
    "            print(driver.current_url)\n",
    "            # Check yes18years page and click it to go to the next page\n",
    "            if response.url.find(\"AgeCheck\") != -1:\n",
    "\n",
    "                # driver = response.request.meta['driver']\n",
    "\n",
    "                js_code = \"document.querySelector(\\\".yes18years\\\").click();\"  # Click to the next page\n",
    "                driver.execute_script(js_code)\n",
    "                \n",
    "                # Wait for the new content to load after the click\n",
    "                # Adjust the timeout as needed\n",
    "                wait = WebDriverWait(driver, 20)\n",
    "                # Wait for goods comment is ready for click\n",
    "                wait.until(ec.presence_of_element_located((By.XPATH, \"//*\")))\n",
    "                \n",
    "                # Get the updated page source and extract product links\n",
    "                updated_content = driver.page_source\n",
    "                response = Selector(scrapy.http.HtmlResponse(url=driver.current_url,\n",
    "                                                             body=updated_content,\n",
    "                                                             encoding='utf-8'))\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            categories = response.css('#bt_category_Content ul li')\n",
    "\n",
    "            tier_dict = {}\n",
    "            threads = []\n",
    "            if len(categories) > 0:\n",
    "                tier_path = response.css(\"#bt_2_layout_NAV\") \n",
    "\n",
    "                tier_index = 1\n",
    "                for tier in tier_path:\n",
    "                    tier_temp_path = tier.css('ul li') \n",
    "\n",
    "                    if tier_temp_path.css(\"::attr(class)\").get() != 'first':\n",
    "                        tier_a = tier_temp_path.css(\"a\").get()\n",
    "                        if tier_a is not None:\n",
    "                            tier_dict[f'tier{tier_index}'] = tier_temp_path.css(\"a::text\").get()\n",
    "                        else:\n",
    "                            tier_dict[f'tier{tier_index}'] = tier_temp_path.css(\"::text\").get()\n",
    "                        tier_index = tier_index+1\n",
    "            print(tier_dict)\n",
    "            if \"tier1\" in tier_dict:\n",
    "                for category in categories:\n",
    "                    self.parse_process(category, tier_dict)\n",
    "                    # thread = threading.Thread(target=self.parse_process, args=(category, tier_dict,))\n",
    "                    # threads.append(thread)\n",
    "                    # thread.start()\n",
    "                # for thread in threads:\n",
    "                    # thread.join()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            self.runner.timeout = False\n",
    "\n",
    "    # Function to process individual category items\n",
    "    def parse_process(self, category, tier_dict):\n",
    "\n",
    "        category_item_a = category.css('a').get()\n",
    "        category_link = \"\"\n",
    "        if category_item_a is not None:\n",
    "            category_link = category.css('li a::attr(href)').get()\n",
    "\n",
    "        current_tier = check_tier_by_url(category_link)\n",
    "\n",
    "        if current_tier == \"tier4\":\n",
    "            tier_dict['tier4'] = category.css(\"a::text\").get()\n",
    "        elif current_tier == \"tier3\":\n",
    "            tier_dict['tier3'] = category.css(\"a::text\").get()\n",
    "\n",
    "        if tier_dict is not None:\n",
    "            category_link = \"https://www.momoshop.com.tw\" + category_link if category_link.find(\"momoshop.com.tw\") == -1 else category_link\n",
    "            message_body = {'tier1_category_name': tier_dict[\"tier1\"],\n",
    "                            'tier2_category_name': tier_dict.get(\"tier2\", \"\"),\n",
    "                            'tier3_category_name': tier_dict.get(\"tier3\", \"\"),\n",
    "                            'tier4_category_name': tier_dict.get(\"tier4\", \"\"),\n",
    "                            # https://ecm.momoshop.com.tw/category/DgrpCategory.jsp?d_code=1704300019&p_orderType=6&showType=chessboardType&sourcePageType=4\n",
    "                            'category_link': category_link.replace(\"ecm.momoshop.com.tw\", \"www.momoshop.com.tw\"),\n",
    "                            'category_type': current_tier}\n",
    "\n",
    "            hash_id = hash_function(str(category_link))\n",
    "            self.start_to_push(current_tier, hash_id, tier_dict, category_link, message_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9f00205-cb8a-48e4-8da5-b727f2c3e4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#============================\n",
    "import datetime\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from crochet import setup\n",
    "import json\n",
    "import threading\n",
    "import momocrawler.settings\n",
    "import momocrawler.middlewares\n",
    "import momocrawler.pipelines\n",
    "# from momoshop_tier4 import MomoshopSpider\n",
    "\n",
    "# Retrieve environment variables\n",
    "queue_tier2_links = 'https://sqs.ap-northeast-1.amazonaws.com/407620147666/aws_crawler_tier2_links.fifo'\n",
    "region = 'ap-northeast-1'\n",
    "# sqs = boto3.client('sqs', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "timeout_seconds = float(os.getenv('TIMEOUT')) if os.getenv('TIMEOUT') is not None else 150  # seconds\n",
    "# Initialize Crochet\n",
    "setup()\n",
    "\n",
    "\n",
    "# Class for running the Lambda spider\n",
    "class LambdaRunner:\n",
    "    target_url = \"\"\n",
    "    receipt_handle = \"\"\n",
    "    timeout = True\n",
    "    input_url = \"\"\n",
    "    category_info = []\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.finished = threading.Event()\n",
    "        self.results = []\n",
    "        if url != \"\":\n",
    "            self.target_url = url\n",
    "\n",
    "    # Function to run the spider\n",
    "    def run_spider(self):\n",
    "        # Create a CrawlerRunner with project settings\n",
    "        settings = get_project_settings()\n",
    "        runner = CrawlerRunner(settings)\n",
    "\n",
    "        # Create an instance of the spider class\n",
    "        spider_cls = MomoshopSpider\n",
    "\n",
    "        # Callback function to handle the spider results\n",
    "        def handle_results(result):\n",
    "            self.results.append(result)\n",
    "\n",
    "            # Check if the spider has finished running\n",
    "            if len(self.results) == 1:\n",
    "                self.finished.set()\n",
    "\n",
    "        # Start the first spider run\n",
    "        deferred = runner.crawl(spider_cls, url=self.target_url, runner=self)\n",
    "        deferred.addCallback(handle_results)\n",
    "\n",
    "        # Start the reactor\n",
    "        runner.join()\n",
    "\n",
    "    # Function to wait for spider completion\n",
    "    def wait_for_completion(self):\n",
    "        self.finished.wait(timeout=timeout_seconds)\n",
    "\n",
    "    # Function to get spider results\n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "\n",
    "\n",
    "# Main handler function for the Lambda\n",
    "def handler(event, context):\n",
    "    try:\n",
    "        print(f\"Starting to crawl: {datetime.datetime.now()}\")\n",
    "        times = 0\n",
    "        if \"statusCode\" not in event:\n",
    "            # Initialize the LambdaRunner with the provided category link\n",
    "\n",
    "            # headers = {\"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "            #            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"}\n",
    "            # response = requests.get('https://myip.com.tw/', headers=headers)\n",
    "            #\n",
    "            # # Check the response status code\n",
    "            # if response.status_code == 200:\n",
    "            #     ip = response.text[response.text.find(\"<font color=green>\") + 18:response.text.find(\"</font></h1>\")]\n",
    "            #     # Request was successful\n",
    "            #     print(ip)\n",
    "            # else:\n",
    "            #     # Request failed\n",
    "            #     print('Request failed with status code:', response.status_code)\n",
    "            runner = LambdaRunner(event[\"category_link\"])\n",
    "            runner.input_url = event[\"category_link\"]\n",
    "            runner.run_spider()\n",
    "            runner.wait_for_completion()\n",
    "\n",
    "            print(f\"End date and time:{datetime.datetime.now()}\")\n",
    "\n",
    "        else:\n",
    "            times = int(event[\"times\"])\n",
    "            if times < 4:\n",
    "                # Initialize the LambdaRunner with the provided category link\n",
    "                runner = LambdaRunner(event[\"category_link\"])\n",
    "                runner.input_url = event[\"category_link\"]\n",
    "                runner.run_spider()\n",
    "                runner.wait_for_completion()\n",
    "                print(f\"End date and time:{datetime.datetime.now()}\")\n",
    "            else:\n",
    "                return {\n",
    "                    'statusCode': 429,\n",
    "                    'body': \"\",\n",
    "                    'times': times,\n",
    "                    \"category_link\": event[\"category_link\"]\n",
    "                }\n",
    "\n",
    "        times = times + 1\n",
    "        if not runner.timeout:\n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': 'Completed!',\n",
    "                'times': times,\n",
    "                # \"category_link\": event[\"category_link\"],\n",
    "                # \"category_info\": runner.category_info\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'statusCode': 408,\n",
    "                'body': event[\"category_link\"],\n",
    "                'times': times,\n",
    "                \"category_link\": event[\"category_link\"]\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({'error': str(e)})\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349768c1-26c7-41c3-8149-9cfbcc93f20f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to crawl: 2023-08-25 09:13:58.111141\n",
      "Starting queue_tier2_links from start_requests\n",
      "self.target_url: https://www.momoshop.com.tw/category/LgrpCategory.jsp?l_code=1205200000&mdiv=1099700000-bt_0_997_13-bt_0_997_13_P103_1_e1&ctype=B\n",
      "Starting to parse_tier2\n"
     ]
    }
   ],
   "source": [
    "# #normal page test\n",
    "# handler({'category_link': 'https://www.momoshop.com.tw/category/LgrpCategory.jsp?l_code=4300100000&mdiv=1099700000-bt_0_997_12-bt_0_997_12_P101_2_e1&ctype=B'\n",
    "#          ,'body': 'https://www.momoshop.com.tw/'},\n",
    "#         \"\")\n",
    "\n",
    "\n",
    "#Agecheck page test\n",
    "\n",
    "handler({'category_link': 'https://www.momoshop.com.tw/category/LgrpCategory.jsp?l_code=1205200000&mdiv=1099700000-bt_0_997_13-bt_0_997_13_P103_1_e1&ctype=B', \n",
    "         'body': 'https://www.momoshop.com.tw/'},\n",
    "        \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b89de-82e1-4b37-bce5-616f15b1e356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb629fe-12b2-49aa-a971-4b873a3bbf21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a253a1-2c52-4a37-a6c1-fab468445b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
