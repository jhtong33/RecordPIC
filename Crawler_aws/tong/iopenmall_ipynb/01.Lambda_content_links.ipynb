{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb192fa3-3cdd-4444-8620-d036121b6941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import botocore\n",
    "from fake_useragent import UserAgent\n",
    "import boto3\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from crochet import setup\n",
    "import threading\n",
    "\n",
    "\n",
    "# Initialize Crochet\n",
    "setup()\n",
    "\n",
    "ua = UserAgent()\n",
    "# region = os.getenv('ap-northeast-1')\n",
    "# sqs = boto3.client('sqs', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "# queue_tier_links = 'https://sqs.ap-northeast-1.amazonaws.com/407620147666/aws_crawler_tier4_links.fifo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61112662-058b-4c0c-85b0-55a65c115590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from scrapy_selenium import SeleniumRequest\n",
    "import botocore\n",
    "import scrapy\n",
    "from fake_useragent import UserAgent\n",
    "import boto3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "ua = UserAgent()\n",
    "# region = os.getenv('ap-northeast-1')\n",
    "# sqs = boto3.client('sqs', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "# s3 = boto3.client('s3', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "export_to_s3 = 'Off'\n",
    "# queue_tier_content_links = 'https://sqs.ap-northeast-1.amazonaws.com/407620147666/aws_crawler_tier5_content_links.fifo'\n",
    "# s3_tier_content_links = os.getenv('S3_tier_content_linkS')\n",
    "# s3_crawler_content_folder = os.getenv('S3_CRAWLER_CONTENT_FOLDER')\n",
    "\n",
    "\n",
    "def hash_function(original_string):\n",
    "    # Create a hash object using SHA-256 algorithm\n",
    "    hasher = hashlib.sha256()\n",
    "\n",
    "    # Convert the input string to bytes (required by hashlib)\n",
    "    input_bytes = original_string.encode('utf-8')\n",
    "\n",
    "    # Update the hash object with the input bytes\n",
    "    hasher.update(input_bytes)\n",
    "\n",
    "    # Get the hexadecimal representation of the hash value\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "\n",
    "\n",
    "class IOpenMallSpider(scrapy.Spider):\n",
    "    name = \"iopenmall\" # modify\n",
    "    allowed_domains = [\"mall.iopenmall.tw/iopen/\"]  # modify\n",
    "    start_urls = \"https://mall.iopenmall.tw/iopen/\"  # modify\n",
    "    user_agent = ua.random\n",
    "    runner = None\n",
    "    category_links = []\n",
    "\n",
    "    def __init__(self, runner=None, **kwargs):\n",
    "        self.runner = runner\n",
    "        super(IOpenMallSpider, self).__init__(**kwargs) # modify\n",
    "\n",
    "    def start_requests(self):\n",
    "        # Start the initial request to fetch category links\n",
    "        print(\"Starting start_requests\")\n",
    "        if self.runner.target_url == \"\":\n",
    "            target_url = json.loads(self.runner.tier4_category_object['Body'])['category_link']\n",
    "        else:\n",
    "            target_url = self.runner.target_url\n",
    "\n",
    "        headers = {\"Host\": urlparse(target_url).netloc,\n",
    "                   'Accept-Encoding': 'gzip, deflate, br',\n",
    "                   'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5',\n",
    "                   'Sec-Fetch-Dest': 'document',\n",
    "                   'Sec-Fetch-Mode': 'navigate',\n",
    "                   'Sec-Fetch-Site': 'none',\n",
    "                   'Upgrade-Insecure-Requests': '1',\n",
    "                   \"Referer\": \"https://www.momoshop.com.tw/main/Main.jsp\",\n",
    "                   \"User-Agent\": ua.random}\n",
    "        print(f'target_url: {target_url}')\n",
    "        yield SeleniumRequest(url=target_url,\n",
    "                              headers=headers,\n",
    "                              callback=self.parse,\n",
    "                              wait_time=15,\n",
    "                              errback=self.error_handle)\n",
    "\n",
    "    def error_handle(self, error):\n",
    "        print(error)\n",
    "        self.runner.timeout = True\n",
    "    \n",
    "\n",
    "    def parse(self, response, **kwargs):\n",
    "        try:\n",
    "            # Parse the main category page and extract links to individual product pages\n",
    "            print(f\"Starting to parse\")\n",
    "            \n",
    "\n",
    "            # Get the number of total pages\n",
    "            less_page = response.css('div.page_number.less_than_thirty_page.pic-flex-center.pic-flex-center-column') # modify\n",
    "            \n",
    "            # modify\n",
    "            if less_page != []:\n",
    "                over_ten_pages = response.css('li.pic-page-num-box ::attr(href)').getall()  \n",
    "                if over_ten_pages != []:\n",
    "                    last_pages_link = over_ten_pages[-1]\n",
    "                    total_pages = last_pages_link.rsplit('page=')[-1]\n",
    "                else:\n",
    "                    total_pages = less_page.css('ul li a ::text').getall()[-1]\n",
    "            else:\n",
    "                over_ten_pages = response.css('li.pic-page-num-box ::attr(href)').getall()\n",
    "                last_pages_link = over_ten_pages[-1]\n",
    "                total_pages = last_pages_link.rsplit('page=')[-1]\n",
    "                \n",
    "            print()\n",
    "            print(f'Total pages: {total_pages}')\n",
    "            \n",
    "            #driver = response.request.meta['driver'] # close to avoid bug\n",
    "            #-------------- add to avoid bugs from jupyter-lab --------------------\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            options.add_argument(\"--single-process\")\n",
    "            options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            options.add_argument(\"blink-settings=imagesEnabled=false\")\n",
    "            options.add_argument(\"--remote-debugging-port=9222\")\n",
    "            options.add_argument(\"--no-zygote\")\n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "            options.add_argument(\"--disable-dev-tools\")\n",
    "    \n",
    "            driver = webdriver.Chrome(options=options)\n",
    "            driver.get(response.url)\n",
    "            #----------------------------------------------------------------------\n",
    "            tier = {} # modify\n",
    "            threads = []\n",
    "            for i in range(1, int(total_pages) + 1):\n",
    "                # For each page, extract product links\n",
    "                if i == 1:\n",
    "                    tier_content_links = response.css(\"div.pic-pds-infobox h2 a::attr(href)\").getall() # modify\n",
    "                    self.set_all_tier(self, tier)\n",
    "                    self.construct_s3_tier_folder(tier)\n",
    "\n",
    "                else:  # If the number of page is greater than 1 then\n",
    "                    # For subsequent pages, click on the page number to load new content and extract links\n",
    "                    \n",
    "                    # modify\n",
    "                    WebDriverWait(driver, 15).until(ec.element_to_be_clickable((By.XPATH, f'//li[@class=\"admweb-v2-page-num-box\"]/a[text()=\"{i}\"]'))).click()\n",
    "\n",
    "                    # Get the updated page source and extract product links\n",
    "                    updated_content = driver.page_source\n",
    "                    updated_response = scrapy.http.HtmlResponse(url=driver.current_url, body=updated_content,\n",
    "                                                                encoding='utf-8')\n",
    "                    \n",
    "                    tier_content_links = updated_response.css(\"div.pic-pds-infobox h2 a::attr(href)\").getall() # modify\n",
    "\n",
    "                # Start the threading to push the product links to SQS and/or S3\n",
    "                self.start_to_push(tier_content_links, tier, threads)\n",
    "\n",
    "            # Wait for all threads to finish before proceeding\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def set_all_tier(self, tier: dict): # modify on 08/30\n",
    "        tier['tier1'] = self.runner.tier1\n",
    "        tier['tier2'] = self.runner.tier2\n",
    "        tier['tier3'] = self.runner.tier3\n",
    "        tier['tier4'] = self.runner.tier4 if self.runner.category_type == 'tier4' else ''\n",
    "        tier['tier5'] = self.runner.tier5 if self.runner.category_type == 'tier5' else ''\n",
    "        tier['category_type'] = self.runner.category_type\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_s3_tier_folder(tier: dict):\n",
    "        # Construct the base folder path\n",
    "        base_path = \"content\"\n",
    "\n",
    "        # Create the base folder if it doesn't exist\n",
    "        # if base_path:\n",
    "            # s3.put_object(Bucket=s3_crawler_content_folder, Key=f\"{base_path}/\")\n",
    "\n",
    "        # Construct the folder structure based on the tiers\n",
    "        for key, value in tier.items():\n",
    "            if value != \"\" and key != 'category_type':\n",
    "                folder_name = value.strip('/')\n",
    "                folder_path = f\"{base_path}/{folder_name}\"\n",
    "                print(folder_path)\n",
    "            # Create the folder if it doesn't exist\n",
    "            # s3.put_object(Bucket=s3_crawler_content_folder, Key=f\"{folder_path}/\")\n",
    "\n",
    "            # Set the current folder path as the base path for the next iteration\n",
    "                base_path = folder_path\n",
    "\n",
    "    def start_to_push(self, tier_content_links, tier, threads):\n",
    "        # For each product link, create a message and push it to SQS and/or S3\n",
    "        for tier_content_link in tier_content_links:\n",
    "            hash_id = hash_function(tier_content_link)\n",
    "            message = {'Id': hash_id, 'MessageGroupId': hash_id,\n",
    "                       'MessageBody': json.dumps({'tier_content_link': tier_content_link, \n",
    "                                                  'tier': tier}, ensure_ascii=False)}\n",
    "\n",
    "#             thread_sqs = threading.Thread(target=self.send_to_sqs, args=(message,))\n",
    "#             threads.append(thread_sqs)\n",
    "#             thread_sqs.start()\n",
    "\n",
    "#             # self.send_to_sqs(message)\n",
    "\n",
    "#             if export_to_s3 is not None and export_to_s3 == \"On\":\n",
    "#                 message_body = {'hash_id': hash_id, 'page_url': tier_content_link}\n",
    "#                 self.send_to_s3(message_body, hash_id)\n",
    "#                 thread_s3 = threading.Thread(target=self.send_to_sqs, args=(message,))\n",
    "#                 threads.append(thread_s3)\n",
    "#                 thread_s3.start()            \n",
    "            \n",
    "            \n",
    "\n",
    "#     @staticmethod\n",
    "#     def send_to_sqs(message):\n",
    "#         # Send the message to SQS\n",
    "#         try:\n",
    "#             # print(message['MessageGroupId'])\n",
    "#             response = sqs.send_message(QueueUrl=queue_tier_content_links,\n",
    "#                                         MessageGroupId=message['MessageGroupId'],\n",
    "#                                         MessageBody=message['MessageBody'],\n",
    "#                                         MessageDeduplicationId=message['MessageGroupId'])\n",
    "#             if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n",
    "#                 print('Fail')\n",
    "#         except Exception as e:\n",
    "#             print(str(e))\n",
    "\n",
    "#     @staticmethod\n",
    "#     def send_to_s3(m, hash_id):\n",
    "#         # Send the message to S3\n",
    "#         try:\n",
    "#             json_file_name = f'{hash_id}.json'\n",
    "#             response = s3.put_object(Bucket=s3_tier_content_links, Key=json_file_name,\n",
    "#                                      Body=json.dumps(m, ensure_ascii=False))\n",
    "#             # check if it's successful\n",
    "#             if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n",
    "#                 print('Fail')\n",
    "#         except Exception as e:\n",
    "#             print(str(e))\n",
    "\n",
    "    def spider_closed(self):\n",
    "        # Handle the spider closing event\n",
    "        self.runner.timeout = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e342bef-71b1-4e1a-ac29-79b1783ff4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LambdaRunner:\n",
    "    target_url = \"\"\n",
    "    receipt_handle = \"\"\n",
    "    tier4_category_object = None\n",
    "    timeout = False\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.finished = threading.Event()\n",
    "        self.results = []\n",
    "        if url != \"\":\n",
    "            self.target_url = url\n",
    "\n",
    "    def run_spider(self):\n",
    "        # Create a CrawlerRunner with project settings\n",
    "        settings = get_project_settings()\n",
    "        runner = CrawlerRunner(settings)\n",
    "        if self.target_url == \"\":\n",
    "            self.get_tier4_url_from_sqs()\n",
    "\n",
    "        # Callback function to handle the spider results\n",
    "        def handle_results(result):\n",
    "            self.results.append(result)\n",
    "\n",
    "            # Check if the spider has finished running\n",
    "            if len(self.results) == 1:\n",
    "                self.finished.set()\n",
    "\n",
    "        # Start the first spider run\n",
    "        deferred = runner.crawl(IOpenMallSpider, runner=self) # modify\n",
    "        deferred.addCallback(handle_results)\n",
    "\n",
    "        # Start the reactor\n",
    "        runner.join()\n",
    "\n",
    "    def wait_for_completion(self):\n",
    "        self.finished.wait()\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "\n",
    "    def get_tier4_url_from_sqs(self):\n",
    "        response = sqs.receive_message(\n",
    "            QueueUrl=queue_tier_links,\n",
    "            MaxNumberOfMessages=1,  # Retrieve 10 messages\n",
    "            WaitTimeSeconds=0  # Maximum time to wait for messages (long polling)\n",
    "        )\n",
    "        messages = response.get('Messages', [])\n",
    "        if messages is not None:\n",
    "            self.tier4_category_object = messages[0]\n",
    "        else:\n",
    "            msg = \"All consumed.\"\n",
    "            print(msg)\n",
    "            return msg\n",
    "\n",
    "        # Delete messages from SQS\n",
    "        for message in messages:\n",
    "            sqs.delete_message(\n",
    "                QueueUrl=queue_tier_links,\n",
    "                ReceiptHandle=message['ReceiptHandle']\n",
    "            )\n",
    "\n",
    "\n",
    "def handler(event, context):\n",
    "    try:\n",
    "        # Check if the function was triggered by an HTTP request or Lambda event\n",
    "        print(f\"Starting to crawl: {datetime.datetime.now()}\")\n",
    "        times = 0\n",
    "        if \"statusCode\" not in event:\n",
    "            # If the function was not triggered by retry\n",
    "            runner = LambdaRunner(\"\")\n",
    "            runner.tier1 = event[\"tier1_category_name\"]    # add on 08/30\n",
    "            runner.tier2 = event[\"tier2_category_name\"]    # add on 08/30\n",
    "            runner.tier3 = event[\"tier3_category_name\"]    # add on 08/30\n",
    "            runner.tier4 = event[\"tier4_category_name\"] if event[\"category_type\"] == 'tier4' else None   # add on 08/30\n",
    "            runner.tier5 = event[\"tier5_category_name\"] if event[\"category_type\"] == 'tier5' else None   # add on 08/30\n",
    "            runner.category_type = event[\"category_type\"]  # add on 08/30\n",
    "            runner.run_spider()\n",
    "            runner.wait_for_completion()\n",
    "            print(f\"End date and time:{datetime.datetime.now()}\")\n",
    "        else:\n",
    "            times = int(event[\"times\"])\n",
    "            if times < 4:\n",
    "                runner = LambdaRunner(event[\"category_link\"])\n",
    "                runner.input_url = event[\"category_link\"]\n",
    "                runner.tier1 = event[\"tier1_category_name\"]    # add on 08/30\n",
    "                runner.tier2 = event[\"tier2_category_name\"]    # add on 08/30\n",
    "                runner.tier3 = event[\"tier3_category_name\"]    # add on 08/30\n",
    "                runner.tier4 = event[\"tier4_category_name\"] if event[\"category_type\"] == 'tier4' else None   # add on 08/30\n",
    "                runner.tier5 = event[\"tier5_category_name\"] if event[\"category_type\"] == 'tier5' else None   # add on 08/30\n",
    "                runner.category_type = event[\"category_type\"]  # add on 08/30\n",
    "                runner.run_spider()\n",
    "                runner.wait_for_completion()\n",
    "                print(f\"End date and time: {datetime.datetime.now()}\")\n",
    "            else:\n",
    "                print(f'Retry too many times, 429: {event[\"category_link\"]}')\n",
    "                # If the retry count is 4 or more, return an HTTP 429 response indicating Too Many Requests\n",
    "                return {\n",
    "                    'statusCode': 429,\n",
    "                    'body': \"Retry too many times\",\n",
    "                    'times': times,\n",
    "                    \"category_link\": event[\"category_link\"]\n",
    "                }\n",
    "\n",
    "        times = times + 1\n",
    "        if not runner.timeout:\n",
    "            print('success')\n",
    "            # If the LambdaRunner completed successfully, return an HTTP 200 response with the completion details\n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': 'Completed!',\n",
    "                'times': times,\n",
    "                # \"category_link\": event[\"category_link\"],\n",
    "                # \"category_info\": runner.category_info\n",
    "            }\n",
    "        else:\n",
    "            print('timeout')\n",
    "            # If the LambdaRunner timed out, return an HTTP 408 response with the category objects\n",
    "            return {\n",
    "                'statusCode': 408,\n",
    "                'times': times,\n",
    "                \"category_link\": json.loads(runner.tier4_category_object['Body'])['category_link']\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f'fail:{e}')\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({'error': str(e)})\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db14a9a-9a7f-4e42-b009-a32adb075c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7cecb5d5-4d62-4776-b74f-541fbf6cac16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to crawl: 2023-08-30 07:42:44.499055\n",
      "Starting start_requests\n",
      "target_url: https://mall.iopenmall.tw/iopen/index.php?action=store_product_sort&prod_sort_ou=456\n",
      "Starting to parse\n",
      "\n",
      "Total pages: 1\n",
      "content/休閒美食\n",
      "content/休閒美食/美食、伴手禮\n",
      "content/休閒美食/美食、伴手禮/乳製品、蛋\n",
      "content/休閒美食/美食、伴手禮/乳製品、蛋/其他\n",
      "End date and time: 2023-08-30 07:42:47.840058\n",
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200, 'body': 'Completed!', 'times': 2}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler({'statusCode': 408,'times': 1, \n",
    "        'tier1_category_name': '休閒美食', \n",
    "         'tier2_category_name': '美食、伴手禮', \n",
    "         'tier3_category_name': '乳製品、蛋', \n",
    "         'tier4_category_name': '其他', \n",
    "         'tier5_category_name': '', \n",
    "         'category_link': 'https://mall.iopenmall.tw/iopen/index.php?action=store_product_sort&prod_sort_ou=456', \n",
    "         'category_type': 'tier4'}, \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628a416-48b8-463c-a69a-7e033ec90c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90cfe6-230a-440f-9ef9-90a9b2e6ae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
