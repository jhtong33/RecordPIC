{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df62e9f4-006f-426a-949b-785acf5978ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !sudo yum install Xvfb -y -y\n",
    "# !pip install scrapy\n",
    "# !pip install fake_useragent\n",
    "# !pip install scrapy-selenium\n",
    "# !pip install selenium==4.9.1\n",
    "# !pip install boto3==1.28.5\n",
    "# !pip install crochet\n",
    "# !pip install awslambdaric\n",
    "# !pip install botocore==1.31.10\n",
    "# #\n",
    "\n",
    "# import os\n",
    "# cmd = '''\n",
    "# sudo yum install atk cups-libs gtk3 libXcomposite alsa-lib libXcursor libXdamage libXext libXi libXrandr libXScrnSaver libXtst pango at-spi2-atk libXt xorg-x11-server-Xvfb xorg-x11-xauth dbus-glib dbus-glib-devel -y\n",
    "# '''\n",
    "# os.system(cmd)\n",
    "\n",
    "\n",
    "# !curl  -Lo \"/tmp/chromedriver.zip\" \"https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip\"\n",
    "# !unzip /tmp/chromedriver.zip -d /opt/ -y\n",
    "# !sudo ln -sf /opt/chromedriver /usr/local/bin/chromedriver\n",
    "\n",
    "# !curl -Lo \"/tmp/chrome-linux.zip\" \"https://www.googleapis.com/download/storage/v1/b/chromium-browser-snapshots/o/Linux_x64%2F1135561%2Fchrome-linux.zip?alt=media\"\n",
    "# !unzip /tmp/chrome-linux.zip -d /opt/ -y\n",
    "# !sudo ln -sf /opt/chrome-linux/chrome /usr/local/bin/chrome\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2d708d4-d035-49a4-b67f-b0a5984eb7ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import botocore\n",
    "from fake_useragent import UserAgent\n",
    "import boto3\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from crochet import setup\n",
    "import threading\n",
    "# from momoshop_link import MomoshopSpider\n",
    "\n",
    "# Initialize Crochet\n",
    "setup()\n",
    "\n",
    "ua = UserAgent()\n",
    "region = 'ap-northeast-1'\n",
    "sqs = boto3.client('sqs', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "# queue_tier4_links = 'https://sqs.ap-northeast-1.amazonaws.com/407620147666/aws_crawler_tier4_links.fifo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0b840aa-1177-4dcf-a9e4-d0e6dfe45410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from scrapy import Selector\n",
    "from scrapy_selenium import SeleniumRequest\n",
    "import botocore\n",
    "import scrapy\n",
    "from fake_useragent import UserAgent\n",
    "import boto3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "\n",
    "ua = UserAgent()\n",
    "# sqs = boto3.client('sqs', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "# s3 = boto3.client('s3', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "region = 'ap-northeast-1'\n",
    "# export_to_s3 = os.getenv('EXPORT_TO_S3')\n",
    "queue_tier_content_links =  'https://sqs.ap-northeast-1.amazonaws.com/407620147666/aws_crawler_tier4_links.fifo'\n",
    "\n",
    "\n",
    "\n",
    "def hash_function(original_string):\n",
    "    # Create a hash object using SHA-256 algorithm\n",
    "    hasher = hashlib.sha256()\n",
    "\n",
    "    # Convert the input string to bytes (required by hashlib)\n",
    "    input_bytes = original_string.encode('utf-8')\n",
    "\n",
    "    # Update the hash object with the input bytes\n",
    "    hasher.update(input_bytes)\n",
    "\n",
    "    # Get the hexadecimal representation of the hash value\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "# add on 08/30\n",
    "def string_clean(text: str):\n",
    "    text = text.replace('\\n', '').replace('\\t', '').replace(',', '').replace('\\r', '').replace(' ', '').replace('$', '').replace('\\xa0', '')\n",
    "    return text \n",
    "\n",
    "# add on 08/30\n",
    "def CJK_cleaner(string):\n",
    "    import re\n",
    "    filters = re.compile('[^0-9a-zA-Z\\\\u4e00-\\\\u9fff]+', re.UNICODE)\n",
    "    return filters.sub(' ', string)\n",
    "\n",
    "class IOpenMallSpider(scrapy.Spider):\n",
    "    name = \"iopenmall\" # modify\n",
    "    allowed_domains = [\"mall.iopenmall.tw/iopen/\"]  # modify\n",
    "    start_urls = \"https://mall.iopenmall.tw/iopen/\" # modify\n",
    "    user_agent = ua.random\n",
    "    runner = None\n",
    "    category_links = []\n",
    "    tier = None\n",
    "\n",
    "    def __init__(self, runner=None, **kwargs):\n",
    "        self.runner = runner\n",
    "        super(IOpenMallSpider, self).__init__(**kwargs) # modify\n",
    "\n",
    "    def start_requests(self):\n",
    "        # Start the initial request to fetch category links\n",
    "        print(\"Starting start_requests\")\n",
    "        \n",
    "        if self.runner.target_url == \"\":\n",
    "            tier_content_link = json.loads(self.runner.tier4_content_object['Body'])['tier_content_link']\n",
    "        else:\n",
    "            tier_content_link = self.runner.target_url\n",
    "\n",
    "        headers = {\"Host\": urlparse(tier_content_link).netloc,\n",
    "                   'Accept-Encoding': 'gzip, deflate, br',\n",
    "                   'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5',\n",
    "                   'Sec-Fetch-Dest': 'document',\n",
    "                   'Sec-Fetch-Mode': 'navigate',\n",
    "                   'Sec-Fetch-Site': 'none',\n",
    "                   'Upgrade-Insecure-Requests': '1',\n",
    "                   \"Referer\": \"https://mall.iopenmall.tw/iopen/\",\n",
    "                   \"User-Agent\": ua.random}\n",
    "        print(f'tier_content_link: {tier_content_link}')\n",
    "\n",
    "        yield SeleniumRequest(url=tier_content_link,\n",
    "                              headers=headers,\n",
    "                              callback=self.parse,\n",
    "                              wait_time=15,\n",
    "                              errback=self.error_handle,\n",
    "                              wait_until=ec.presence_of_element_located((By.CSS_SELECTOR, '.first.selected'))\n",
    "                              )\n",
    "\n",
    "    # modify on 08/30\n",
    "    @staticmethod\n",
    "    def set_all_tier(self, tier: dict):\n",
    "        tier['tier1'] = self.runner.tier1\n",
    "        tier['tier2'] = self.runner.tier2\n",
    "        tier['tier3'] = self.runner.tier3\n",
    "        tier['tier4'] = self.runner.tier4 if self.runner.category_type == 'tier4' else ''\n",
    "        tier['tier5'] = self.runner.tier5 if self.runner.category_type == 'tier5' else ''\n",
    "        tier['category_type'] = self.runner.category_type\n",
    "        print(tier)\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_s3_tier_folder(content_tier: dict):\n",
    "        # Construct the base folder path\n",
    "        base_path = \"content\"\n",
    "\n",
    "        # Create the base folder if it doesn't exist\n",
    "        if base_path:\n",
    "            s3.put_object(Bucket=s3_crawler_content_folder, Key=f\"{base_path}/\")\n",
    "\n",
    "        # Construct the folder structure based on the tiers\n",
    "        for key, value in content_tier.items():\n",
    "\n",
    "            folder_name = value.strip('/')\n",
    "            folder_path = f\"{base_path}/{folder_name}\"\n",
    "            print(folder_path)\n",
    "            # Create the folder if it doesn't exist\n",
    "            # s3.put_object(Bucket=s3_crawler_content_folder, Key=f\"{folder_path}/\")\n",
    "\n",
    "            # Set the current folder path as the base path for the next iteration\n",
    "            # base_path = folder_path\n",
    "\n",
    "    def error_handle(self, error):\n",
    "        print(error)\n",
    "        self.runner.timeout = True\n",
    "\n",
    "    def parse(self, response, **kwargs):\n",
    "        try:\n",
    "            # Parse the main category page and extract links to individual product pages\n",
    "            print(f\"Starting to parse\")\n",
    "            selector = Selector(response)\n",
    "            \n",
    "            # driver = response.request.meta['driver']\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            options.add_argument(\"--single-process\")\n",
    "            options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            options.add_argument(\"blink-settings=imagesEnabled=false\")\n",
    "            options.add_argument(\"--remote-debugging-port=9222\")\n",
    "            options.add_argument(\"--no-zygote\")\n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "            options.add_argument(\"--disable-dev-tools\")\n",
    "    \n",
    "            driver = webdriver.Chrome(options=options)\n",
    "            driver.get(response.url)\n",
    "            content_info_tier = {}                     # modify on 08/30\n",
    "            self.set_all_tier(self, content_info_tier) # modify on 08/30\n",
    "            # self.construct_s3_tier_folder(content_info_tier)\n",
    "            \n",
    "            # 品牌名稱\n",
    "            item_brand = selector.css(\"div.header_logo ::attr(title)\").get(None) # modify\n",
    "            \n",
    "            # 促銷價\n",
    "            now_price = None\n",
    "\n",
    "            # 品牌名稱\n",
    "            past_price = None\n",
    "\n",
    "            # 折扣後價格 \n",
    "            price_list = selector.css(\".Product_price ::attr(class)\").getall()  # modify\n",
    "            \n",
    "            # modify\n",
    "            if any(\"now_price\" in s for s in price_list) :\n",
    "                now_price = string_clean(selector.css(\".now_price ::text\").getall()[-1])\n",
    "                now_price = int(now_price)\n",
    "\n",
    "            # modify\n",
    "            if any(\"was_price\" in s for s in price_list) :\n",
    "                past_price = selector.css(\"span.was_price ::text\").getall()[-1]\n",
    "                past_price = string_clean(past_price)\n",
    "                if past_price != '':\n",
    "                    past_price = int(past_price)\n",
    "\n",
    "\n",
    "            # 商品編號\n",
    "            item_no = selector.css(\"div.Item_number span ::text\").getall()[1] # modify\n",
    "  \n",
    "            # 商品名稱\n",
    "            item_name = string_clean(selector.css(\"h1.Product_name ::text\").get()) # modify\n",
    "            \n",
    "            # 商品介紹\n",
    "            item_note = \"//\".join(selector.css(\"div.Product_notes p::text\").extract()) # modify\n",
    "            \n",
    "            # 付款方式\n",
    "            pay_method  = string_clean(selector.css(\"#pay_method ::text\").get()) # modify on 08/30\n",
    "            \n",
    "            # 運送方式\n",
    "            delivery_method  = string_clean(selector.css(\"#delivery_method ::text\").get()) # modify\n",
    "            \n",
    "            # 活動 # modify\n",
    "            activity = selector.css(\".discountbox ul li ul li ::text\").getall()\n",
    "            for i in range(activity.count('折價券')):\n",
    "                activity.remove('折價券') \n",
    "\n",
    "            # 商品特色\n",
    "            item_feature = CJK_cleaner(\"//\".join(selector.css('#content1 p ::text').extract())) # modify\n",
    "            \n",
    "            # 商品評價 # modify\n",
    "            if selector.css(\".comment_total_amount_num ::text\").get() is not None:\n",
    "                goods_commend = {\"count\": int(selector.css(\".comment_total_amount_num ::text\").get())}\n",
    "            else:\n",
    "                goods_commend = {\"count\": 0}\n",
    "\n",
    "                \n",
    "            # 評論內容 # modify\n",
    "            review_card_list = []\n",
    "            # If there's any goods commend then fetch all of them\n",
    "            if goods_commend[\"count\"] > 0:\n",
    "                goods_commend[\"indicator_average_value\"] = float(selector.css('.comment_total_grade_num ::text').get())\n",
    "                # Fetch the all comments in the first page\n",
    "                self.fetch_goods_commend(selector, review_card_list)\n",
    "            \n",
    "            # 店家資訊 # modify\n",
    "            store_seller_list = []\n",
    "            self.fetch_seller(selector, store_seller_list)\n",
    "            store_info = store_seller_list\n",
    "\n",
    "            # modify\n",
    "            content_info = {'item_brand': item_brand,\n",
    "                            'item_no': item_no,\n",
    "                            'item_name': item_name,\n",
    "                            'item_note': item_note,\n",
    "                            'now_price': now_price,\n",
    "                            'past_price': past_price,\n",
    "                            'activity': activity,\n",
    "                            'payway' : pay_method,\n",
    "                            'deliveryway' : delivery_method, \n",
    "                            'item_feature': item_feature,\n",
    "                            'store_seller': store_info,\n",
    "                            'goods_commend': goods_commend,\n",
    "                            'goods_commend_review_card_list': review_card_list,\n",
    "                            'data_date': datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "                            }\n",
    "            print(content_info)\n",
    "            \n",
    "            with open('output.json', 'w') as json_file:\n",
    "                json_file.write(json.dumps(content_info, ensure_ascii=False))\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "    # modify\n",
    "    @staticmethod\n",
    "    def fetch_goods_commend(selector, review_card_list):\n",
    "        for review_card in selector.css('.comment_data'):\n",
    "            review_card_score_temp = review_card.css(\".comment_grade_star img ::attr(src)\").get()\n",
    "            review_card_score = int((review_card_score_temp.rsplit('_')[-1]).rsplit('.')[0])\n",
    "            review_card_comment = (review_card.css(\".customer_comment_reply p ::text\").get())\n",
    "            if review_card_comment is not None: \n",
    "                review_card_comment = string_clean(review_card_comment)\n",
    " \n",
    "            review_card_list.append({\n",
    "                'review_card_score': review_card_score,\n",
    "                'review_card_comment': review_card_comment\n",
    "            })\n",
    "    \n",
    "    # add \n",
    "    @staticmethod\n",
    "    def fetch_seller(selector, store_seller_list):\n",
    "        seller_info_card = selector.css(\"div.iopen-footer-spec ul.header_spec_v2 li\")\n",
    "        for seller_card in seller_info_card:\n",
    "            \n",
    "            item = (seller_card.css(\"::text\").get()).rsplit(':')[0]\n",
    "            info = str(seller_card.css(\"span ::text\").get()).replace('\\xa0', '')\n",
    "            store_seller_list.append({\n",
    "                item: info,\n",
    "            })\n",
    "\n",
    "\n",
    "    def start_to_push(self, tier_content_links, threads):\n",
    "        # For each product link, create a message and push it to SQS and/or S3\n",
    "        for tier_content_link in tier_content_links:\n",
    "            hash_id = hash_function(tier_content_link)\n",
    "            message = {'Id': hash_id, 'MessageGroupId': hash_id,\n",
    "                       'MessageBody': tier_content_link}\n",
    "\n",
    "#             thread_sqs = threading.Thread(target=self.send_to_sqs, args=(message,))\n",
    "#             threads.append(thread_sqs)\n",
    "#             thread_sqs.start()\n",
    "\n",
    "#             # self.send_to_sqs(message)\n",
    "\n",
    "#             if export_to_s3 is not None and export_to_s3 == \"On\":\n",
    "#                 message_body = {'hash_id': hash_id, 'page_url': tier_content_link}\n",
    "#                 self.send_to_s3(message_body, hash_id)\n",
    "#                 thread_s3 = threading.Thread(target=self.send_to_sqs, args=(message,))\n",
    "#                 threads.append(thread_s3)\n",
    "#                 thread_s3.start()\n",
    "\n",
    "#     @staticmethod\n",
    "#     def send_to_sqs(message):\n",
    "#         # Send the message to SQS\n",
    "#         try:\n",
    "#             # print(message['MessageGroupId'])\n",
    "#             response = sqs.send_message(QueueUrl=queue_tier_content_links,\n",
    "#                                         MessageGroupId=message['MessageGroupId'],\n",
    "#                                         MessageBody=message['MessageBody'],\n",
    "#                                         MessageDeduplicationId=message['MessageGroupId'])\n",
    "#             if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n",
    "#                 print('Fail')\n",
    "#         except Exception as e:\n",
    "#             print(str(e))\n",
    "\n",
    "#     @staticmethod\n",
    "#     def send_to_s3(m, hash_id):\n",
    "#         # Send the message to S3\n",
    "#         try:\n",
    "#             json_file_name = f'{hash_id}.json'\n",
    "#             response = s3.put_object(Bucket=s3_tier_content_links, Key=json_file_name,\n",
    "#                                      Body=json.dumps(m, ensure_ascii=False))\n",
    "#             # check if it's successful\n",
    "#             if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n",
    "#                 print('Fail')\n",
    "#         except Exception as e:\n",
    "#             print(str(e))\n",
    "\n",
    "    def spider_closed(self):\n",
    "        # Handle the spider closing event\n",
    "        self.runner.timeout = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ec3f74c-1e1e-4c6c-998b-01b6a556710b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LambdaRunner:\n",
    "    target_url = \"\"\n",
    "    receipt_handle = \"\"\n",
    "    tier4_content_object = None\n",
    "    timeout = False\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.finished = threading.Event()\n",
    "        self.results = []\n",
    "        if url != \"\":\n",
    "            self.target_url = url\n",
    "\n",
    "    def run_spider(self):\n",
    "        # Create a CrawlerRunner with project settings\n",
    "        settings = get_project_settings()\n",
    "        runner = CrawlerRunner(settings)\n",
    "        if self.target_url == \"\":\n",
    "            self.get_tier4_content_url_from_sqs()\n",
    "\n",
    "        # Callback function to handle the spider results\n",
    "        def handle_results(result):\n",
    "            self.results.append(result)\n",
    "\n",
    "            # Check if the spider has finished running\n",
    "            if len(self.results) == 1:\n",
    "                self.finished.set()\n",
    "\n",
    "        # Start the first spider run\n",
    "        deferred = runner.crawl(IOpenMallSpider, self) #modify\n",
    "        deferred.addCallback(handle_results)\n",
    "\n",
    "        # Start the reactor\n",
    "        runner.join()\n",
    "\n",
    "    def wait_for_completion(self):\n",
    "        self.finished.wait()\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "\n",
    "    def get_tier4_content_url_from_sqs(self):\n",
    "        response = sqs.receive_message(\n",
    "            QueueUrl=queue_tier_content_links,\n",
    "            MaxNumberOfMessages=1,  # Retrieve 10 messages\n",
    "            WaitTimeSeconds=0  # Maximum time to wait for messages (long polling)\n",
    "        )\n",
    "        messages = response.get('Messages', [])\n",
    "        if messages is not None:\n",
    "            self.tier4_content_object = messages[0]\n",
    "        else:\n",
    "            msg = \"All consumed.\"\n",
    "            print(msg)\n",
    "            return msg\n",
    "\n",
    "        # Delete messages from SQS\n",
    "        for message in messages:\n",
    "            sqs.delete_message(\n",
    "                QueueUrl=queue_tier_content_links,\n",
    "                ReceiptHandle=message['ReceiptHandle']\n",
    "            )\n",
    "\n",
    "\n",
    "def handler(event, context):\n",
    "    try:\n",
    "        # Check if the function was triggered by an HTTP request or Lambda event\n",
    "        print(f\"Starting to crawl:{datetime.datetime.now()}\")\n",
    "        times = 0\n",
    "        if \"statusCode\" not in event:\n",
    "            # If the function was not triggered by retry\n",
    "            runner = LambdaRunner(\"\")\n",
    "            runner.tier1 = event[\"tier\"][\"tier1\"]    # add on 08/30\n",
    "            runner.tier2 = event[\"tier\"][\"tier2\"]    # add on 08/30 \n",
    "            runner.tier3 = event[\"tier\"][\"tier3\"]    # add on 08/30\n",
    "            runner.tier4 = event[\"tier\"][\"tier4\"] if event[\"tier\"][\"category_type\"] == 'tier4' else None   # add on 08/30\n",
    "            runner.tier5 = event[\"tier\"][\"tier5\"] if event[\"tier\"][\"category_type\"] == 'tier5' else None   # add on 08/30\n",
    "            runner.category_type = event[\"tier\"][\"category_type\"]  # add on 08/30\n",
    "            runner.run_spider()\n",
    "            runner.wait_for_completion()\n",
    "            print(f\"End date and time: {datetime.datetime.now()}\")\n",
    "        else:\n",
    "            times = int(event[\"times\"])\n",
    "            if times < 4:\n",
    "                runner = LambdaRunner(event[\"tier_content_link\"])\n",
    "                runner.input_url = event[\"tier_content_link\"]\n",
    "                runner.tier1 = event[\"tier\"][\"tier1\"]    # add on 08/30\n",
    "                runner.tier2 = event[\"tier\"][\"tier2\"]    # add on 08/30\n",
    "                runner.tier3 = event[\"tier\"][\"tier3\"]    # add on 08/30\n",
    "                runner.tier4 = event[\"tier\"][\"tier4\"] if event[\"tier\"][\"category_type\"] == 'tier4' else None   # add on 08/30\n",
    "                runner.tier5 = event[\"tier\"][\"tier5\"] if event[\"tier\"][\"category_type\"] == 'tier5' else None   # add on 08/30\n",
    "                runner.category_type = event[\"tier\"][\"category_type\"]  # add on 08/30\n",
    "                runner.run_spider()\n",
    "                runner.wait_for_completion()\n",
    "                print(f\"End date and time: {datetime.datetime.now()}\")\n",
    "            else:\n",
    "                print(f'Retry too many times, 429: {event[\"tier_content_link\"]}')\n",
    "                # If the retry count is 4 or more, return an HTTP 429 response indicating Too Many Requests\n",
    "                return {\n",
    "                    'statusCode': 429,\n",
    "                    'body': \"\",\n",
    "                    'times': times,\n",
    "                    \"category_link\": event[\"tier_content_link\"]\n",
    "                }\n",
    "\n",
    "        times = times + 1\n",
    "        if not runner.timeout:\n",
    "            print(\"success\")\n",
    "            # If the LambdaRunner completed successfully, return an HTTP 200 response with the completion details\n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': 'Completed!',\n",
    "                'times': times\n",
    "            }\n",
    "        else:\n",
    "            print('timeout')\n",
    "            # If the LambdaRunner timed out, return an HTTP 408 response with the category objects\n",
    "            return {\n",
    "                'statusCode': 408,\n",
    "                'times': times,\n",
    "                \"category_link\": json.loads(runner.tier4_content_object['Body'])['tier_content_link']\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f'fail:{e}')\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({'error': str(e)})\n",
    "        }\n",
    "\n",
    "# #\n",
    "# if __name__ == '__main__':\n",
    "#     handler(\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed63adf8-a031-429f-9044-fa4f85980663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to crawl:2023-08-30 07:58:23.372268\n",
      "Starting start_requests\n",
      "tier_content_link: https://mall.iopenmall.tw/016725/index.php?action=product_detail&prod_no=P1672501667466\n",
      "Starting to parse\n",
      "{'tier1': '流行時尚', 'tier2': '女鞋', 'tier3': '跟鞋', 'tier4': '', 'tier5': '', 'category_type': 'tier3'}\n",
      "{'item_brand': '韓版女裝．女鞋', 'item_no': 'P1672501667466', 'item_name': '【leleshop】36-40小皮鞋高跟涼拖鞋素色粗跟高跟鞋瑪莉珍鞋懶人鞋牛津鞋方頭高跟鞋#3286', 'item_note': '🎀粗跟小皮鞋 #3268//🔶顏色：黑色、米色、豆沙色//🔸底厚度8公分//🔶尺寸：//36（適合腳長23）//37（適合腳長23.5）//38（適合腳長24）//39（適合腳長24.5）//40（適合腳長25）//🌸版型：正常版//⭕️超甜價$550❤️', 'now_price': 550, 'past_price': '', 'activity': ['0829-0830-滿$399現折$40'], 'payway': '超商取貨付款', 'deliveryway': '常溫7-ELEVEN店到店取貨付款', 'item_feature': ' 粗跟小皮鞋 3268 每個顏色都超美的啦 版媽建議可以直接包色輪流替換 顏色 黑色 米色 豆沙色 底厚度8公分 尺寸 36 適合腳長23 37 適合腳長23 5 38 適合腳長24 39 適合腳長24 5 40 適合腳長25 版型 正常版 超甜價 550 每台顯示器皆不同 依實品顏色為主 高標完美主義者請繞道而行 下單前請注意 現貨 預購 設定7天出貨只是擔心蝦皮 3天後自動取消訂單 商品基本上2 3天就可以收到 如果需要訂購鞋子請等 3 15天 怕等待的水水可先詢問在下單 對我們的商品和服務滿意的 麻煩您給五星好評哦 有不滿意的地方麻煩您在聊聊上面聯繫我們 ', 'store_seller': [{'商品': '87'}, {'加入時間': '2023-07-07'}, {'評價': '5.0/5.0'}, {'購買人次': '1人'}], 'goods_commend': {'count': 0}, 'goods_commend_review_card_list': [], 'data_date': '20230830'}\n",
      "End date and time: 2023-08-30 07:58:26.449031\n",
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200, 'body': 'Completed!', 'times': 2}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler({'statusCode': 200,'times': 1, \n",
    "    \"tier_content_link\": \"https://mall.iopenmall.tw/016725/index.php?action=product_detail&prod_no=P1672501667466\", \n",
    "    \"tier\": {\"tier1\": \"流行時尚\", \"tier2\": \"女鞋\", \"tier3\": \"跟鞋\", \"tier4\": '', \"tier5\": '', \"category_type\": \"tier3\"}}, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b439ba-6d6a-4366-a22a-a15b4ab81154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4faf03-682d-455a-b17b-585fd78dc124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
