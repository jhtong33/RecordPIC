{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df62e9f4-006f-426a-949b-785acf5978ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !sudo yum install Xvfb -y -y\n",
    "# !pip install scrapy\n",
    "# !pip install fake_useragent\n",
    "# !pip install scrapy-selenium\n",
    "# !pip install selenium==4.9.1\n",
    "# !pip install boto3==1.28.5\n",
    "# !pip install crochet\n",
    "# !pip install awslambdaric\n",
    "# !pip install botocore==1.31.10\n",
    "# #\n",
    "\n",
    "# import os\n",
    "# cmd = '''\n",
    "# sudo yum install atk cups-libs gtk3 libXcomposite alsa-lib libXcursor libXdamage libXext libXi libXrandr libXScrnSaver libXtst pango at-spi2-atk libXt xorg-x11-server-Xvfb xorg-x11-xauth dbus-glib dbus-glib-devel -y\n",
    "# '''\n",
    "# os.system(cmd)\n",
    "\n",
    "\n",
    "# !curl  -Lo \"/tmp/chromedriver.zip\" \"https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip\"\n",
    "# !unzip /tmp/chromedriver.zip -d /opt/ -y\n",
    "# !sudo ln -sf /opt/chromedriver /usr/local/bin/chromedriver\n",
    "\n",
    "# !curl -Lo \"/tmp/chrome-linux.zip\" \"https://www.googleapis.com/download/storage/v1/b/chromium-browser-snapshots/o/Linux_x64%2F1135561%2Fchrome-linux.zip?alt=media\"\n",
    "# !unzip /tmp/chrome-linux.zip -d /opt/ -y\n",
    "# !sudo ln -sf /opt/chrome-linux/chrome /usr/local/bin/chrome\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2d708d4-d035-49a4-b67f-b0a5984eb7ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import botocore\n",
    "from fake_useragent import UserAgent\n",
    "import boto3\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from crochet import setup\n",
    "import threading\n",
    "# from momoshop_link import MomoshopSpider\n",
    "\n",
    "# Initialize Crochet\n",
    "setup()\n",
    "\n",
    "ua = UserAgent()\n",
    "region = 'ap-northeast-1'\n",
    "sqs = boto3.client('sqs', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "# queue_tier4_links = 'https://sqs.ap-northeast-1.amazonaws.com/407620147666/aws_crawler_tier4_links.fifo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0b840aa-1177-4dcf-a9e4-d0e6dfe45410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from scrapy import Selector\n",
    "from scrapy_selenium import SeleniumRequest\n",
    "import botocore\n",
    "import scrapy\n",
    "from fake_useragent import UserAgent\n",
    "import boto3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "\n",
    "ua = UserAgent()\n",
    "# sqs = boto3.client('sqs', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "# s3 = boto3.client('s3', region_name=region, config=botocore.client.Config(max_pool_connections=500))\n",
    "region = 'ap-northeast-1'\n",
    "# export_to_s3 = os.getenv('EXPORT_TO_S3')\n",
    "queue_tier_content_links =  'https://sqs.ap-northeast-1.amazonaws.com/407620147666/aws_crawler_tier4_links.fifo'\n",
    "\n",
    "\n",
    "\n",
    "def hash_function(original_string):\n",
    "    # Create a hash object using SHA-256 algorithm\n",
    "    hasher = hashlib.sha256()\n",
    "\n",
    "    # Convert the input string to bytes (required by hashlib)\n",
    "    input_bytes = original_string.encode('utf-8')\n",
    "\n",
    "    # Update the hash object with the input bytes\n",
    "    hasher.update(input_bytes)\n",
    "\n",
    "    # Get the hexadecimal representation of the hash value\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "# add on 08/30\n",
    "def string_clean(text: str):\n",
    "    text = text.replace('\\n', '').replace('\\t', '').replace(',', '').replace('\\r', '').replace(' ', '').replace('$', '').replace('\\xa0', '')\n",
    "    return text \n",
    "\n",
    "# add on 08/30\n",
    "def CJK_cleaner(string):\n",
    "    import re\n",
    "    filters = re.compile('[^0-9a-zA-Z\\\\u4e00-\\\\u9fff]+', re.UNICODE)\n",
    "    return filters.sub(' ', string)\n",
    "\n",
    "class IOpenMallSpider(scrapy.Spider):\n",
    "    name = \"iopenmall\" # modify\n",
    "    allowed_domains = [\"mall.iopenmall.tw/iopen/\"]  # modify\n",
    "    start_urls = \"https://mall.iopenmall.tw/iopen/\" # modify\n",
    "    user_agent = ua.random\n",
    "    runner = None\n",
    "    category_links = []\n",
    "    tier = None\n",
    "\n",
    "    def __init__(self, runner=None, **kwargs):\n",
    "        self.runner = runner\n",
    "        super(IOpenMallSpider, self).__init__(**kwargs) # modify\n",
    "\n",
    "    def start_requests(self):\n",
    "        # Start the initial request to fetch category links\n",
    "        print(\"Starting start_requests\")\n",
    "        \n",
    "        if self.runner.target_url == \"\":\n",
    "            tier_content_link = json.loads(self.runner.tier4_content_object['Body'])['tier_content_link']\n",
    "        else:\n",
    "            tier_content_link = self.runner.target_url\n",
    "\n",
    "        headers = {\"Host\": urlparse(tier_content_link).netloc,\n",
    "                   'Accept-Encoding': 'gzip, deflate, br',\n",
    "                   'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5',\n",
    "                   'Sec-Fetch-Dest': 'document',\n",
    "                   'Sec-Fetch-Mode': 'navigate',\n",
    "                   'Sec-Fetch-Site': 'none',\n",
    "                   'Upgrade-Insecure-Requests': '1',\n",
    "                   \"Referer\": \"https://mall.iopenmall.tw/iopen/\",\n",
    "                   \"User-Agent\": ua.random}\n",
    "        print(f'tier_content_link: {tier_content_link}')\n",
    "\n",
    "        yield SeleniumRequest(url=tier_content_link,\n",
    "                              headers=headers,\n",
    "                              callback=self.parse,\n",
    "                              wait_time=15,\n",
    "                              errback=self.error_handle,\n",
    "                              wait_until=ec.presence_of_element_located((By.CSS_SELECTOR, '.first.selected'))\n",
    "                              )\n",
    "\n",
    "    # modify on 08/30\n",
    "    @staticmethod\n",
    "    def set_all_tier(self, tier: dict):\n",
    "        tier['tier1'] = self.runner.tier1\n",
    "        tier['tier2'] = self.runner.tier2\n",
    "        tier['tier3'] = self.runner.tier3\n",
    "        tier['tier4'] = self.runner.tier4 if self.runner.category_type == 'tier4' else ''\n",
    "        tier['tier5'] = self.runner.tier5 if self.runner.category_type == 'tier5' else ''\n",
    "        tier['category_type'] = self.runner.category_type\n",
    "        print(tier)\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_s3_tier_folder(content_tier: dict):\n",
    "        # Construct the base folder path\n",
    "        base_path = \"content\"\n",
    "\n",
    "        # Create the base folder if it doesn't exist\n",
    "        if base_path:\n",
    "            s3.put_object(Bucket=s3_crawler_content_folder, Key=f\"{base_path}/\")\n",
    "\n",
    "        # Construct the folder structure based on the tiers\n",
    "        for key, value in content_tier.items():\n",
    "\n",
    "            folder_name = value.strip('/')\n",
    "            folder_path = f\"{base_path}/{folder_name}\"\n",
    "            print(folder_path)\n",
    "            # Create the folder if it doesn't exist\n",
    "            # s3.put_object(Bucket=s3_crawler_content_folder, Key=f\"{folder_path}/\")\n",
    "\n",
    "            # Set the current folder path as the base path for the next iteration\n",
    "            # base_path = folder_path\n",
    "\n",
    "    def error_handle(self, error):\n",
    "        print(error)\n",
    "        self.runner.timeout = True\n",
    "\n",
    "    def parse(self, response, **kwargs):\n",
    "        try:\n",
    "            # Parse the main category page and extract links to individual product pages\n",
    "            print(f\"Starting to parse\")\n",
    "            selector = Selector(response)\n",
    "            \n",
    "            # driver = response.request.meta['driver']\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            options.add_argument(\"--single-process\")\n",
    "            options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            options.add_argument(\"blink-settings=imagesEnabled=false\")\n",
    "            options.add_argument(\"--remote-debugging-port=9222\")\n",
    "            options.add_argument(\"--no-zygote\")\n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "            options.add_argument(\"--disable-dev-tools\")\n",
    "    \n",
    "            driver = webdriver.Chrome(options=options)\n",
    "            driver.get(response.url)\n",
    "            content_info_tier = {}                     # modify on 08/30\n",
    "            self.set_all_tier(self, content_info_tier) # modify on 08/30\n",
    "            # self.construct_s3_tier_folder(content_info_tier)\n",
    "            \n",
    "            # å“ç‰Œåç¨±\n",
    "            item_brand = selector.css(\"div.header_logo ::attr(title)\").get(None) # modify\n",
    "            \n",
    "            # ä¿ƒéŠ·åƒ¹\n",
    "            now_price = None\n",
    "\n",
    "            # å“ç‰Œåç¨±\n",
    "            past_price = None\n",
    "\n",
    "            # æŠ˜æ‰£å¾Œåƒ¹æ ¼ \n",
    "            price_list = selector.css(\".Product_price ::attr(class)\").getall()  # modify\n",
    "            \n",
    "            # modify\n",
    "            if any(\"now_price\" in s for s in price_list) :\n",
    "                now_price = string_clean(selector.css(\".now_price ::text\").getall()[-1])\n",
    "                now_price = int(now_price)\n",
    "\n",
    "            # modify\n",
    "            if any(\"was_price\" in s for s in price_list) :\n",
    "                past_price = selector.css(\"span.was_price ::text\").getall()[-1]\n",
    "                past_price = string_clean(past_price)\n",
    "                if past_price != '':\n",
    "                    past_price = int(past_price)\n",
    "\n",
    "\n",
    "            # å•†å“ç·¨è™Ÿ\n",
    "            item_no = selector.css(\"div.Item_number span ::text\").getall()[1] # modify\n",
    "  \n",
    "            # å•†å“åç¨±\n",
    "            item_name = string_clean(selector.css(\"h1.Product_name ::text\").get()) # modify\n",
    "            \n",
    "            # å•†å“ä»‹ç´¹\n",
    "            item_note = \"//\".join(selector.css(\"div.Product_notes p::text\").extract()) # modify\n",
    "            \n",
    "            # ä»˜æ¬¾æ–¹å¼\n",
    "            pay_method  = string_clean(selector.css(\"#pay_method ::text\").get()) # modify on 08/30\n",
    "            \n",
    "            # é‹é€æ–¹å¼\n",
    "            delivery_method  = string_clean(selector.css(\"#delivery_method ::text\").get()) # modify\n",
    "            \n",
    "            # æ´»å‹• # modify\n",
    "            activity = selector.css(\".discountbox ul li ul li ::text\").getall()\n",
    "            for i in range(activity.count('æŠ˜åƒ¹åˆ¸')):\n",
    "                activity.remove('æŠ˜åƒ¹åˆ¸') \n",
    "\n",
    "            # å•†å“ç‰¹è‰²\n",
    "            item_feature = CJK_cleaner(\"//\".join(selector.css('#content1 p ::text').extract())) # modify\n",
    "            \n",
    "            # å•†å“è©•åƒ¹ # modify\n",
    "            if selector.css(\".comment_total_amount_num ::text\").get() is not None:\n",
    "                goods_commend = {\"count\": int(selector.css(\".comment_total_amount_num ::text\").get())}\n",
    "            else:\n",
    "                goods_commend = {\"count\": 0}\n",
    "\n",
    "                \n",
    "            # è©•è«–å…§å®¹ # modify\n",
    "            review_card_list = []\n",
    "            # If there's any goods commend then fetch all of them\n",
    "            if goods_commend[\"count\"] > 0:\n",
    "                goods_commend[\"indicator_average_value\"] = float(selector.css('.comment_total_grade_num ::text').get())\n",
    "                # Fetch the all comments in the first page\n",
    "                self.fetch_goods_commend(selector, review_card_list)\n",
    "            \n",
    "            # åº—å®¶è³‡è¨Š # modify\n",
    "            store_seller_list = []\n",
    "            self.fetch_seller(selector, store_seller_list)\n",
    "            store_info = store_seller_list\n",
    "\n",
    "            # modify\n",
    "            content_info = {'item_brand': item_brand,\n",
    "                            'item_no': item_no,\n",
    "                            'item_name': item_name,\n",
    "                            'item_note': item_note,\n",
    "                            'now_price': now_price,\n",
    "                            'past_price': past_price,\n",
    "                            'activity': activity,\n",
    "                            'payway' : pay_method,\n",
    "                            'deliveryway' : delivery_method, \n",
    "                            'item_feature': item_feature,\n",
    "                            'store_seller': store_info,\n",
    "                            'goods_commend': goods_commend,\n",
    "                            'goods_commend_review_card_list': review_card_list,\n",
    "                            'data_date': datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "                            }\n",
    "            print(content_info)\n",
    "            \n",
    "            with open('output.json', 'w') as json_file:\n",
    "                json_file.write(json.dumps(content_info, ensure_ascii=False))\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "    # modify\n",
    "    @staticmethod\n",
    "    def fetch_goods_commend(selector, review_card_list):\n",
    "        for review_card in selector.css('.comment_data'):\n",
    "            review_card_score_temp = review_card.css(\".comment_grade_star img ::attr(src)\").get()\n",
    "            review_card_score = int((review_card_score_temp.rsplit('_')[-1]).rsplit('.')[0])\n",
    "            review_card_comment = (review_card.css(\".customer_comment_reply p ::text\").get())\n",
    "            if review_card_comment is not None: \n",
    "                review_card_comment = string_clean(review_card_comment)\n",
    " \n",
    "            review_card_list.append({\n",
    "                'review_card_score': review_card_score,\n",
    "                'review_card_comment': review_card_comment\n",
    "            })\n",
    "    \n",
    "    # add \n",
    "    @staticmethod\n",
    "    def fetch_seller(selector, store_seller_list):\n",
    "        seller_info_card = selector.css(\"div.iopen-footer-spec ul.header_spec_v2 li\")\n",
    "        for seller_card in seller_info_card:\n",
    "            \n",
    "            item = (seller_card.css(\"::text\").get()).rsplit(':')[0]\n",
    "            info = str(seller_card.css(\"span ::text\").get()).replace('\\xa0', '')\n",
    "            store_seller_list.append({\n",
    "                item: info,\n",
    "            })\n",
    "\n",
    "\n",
    "    def start_to_push(self, tier_content_links, threads):\n",
    "        # For each product link, create a message and push it to SQS and/or S3\n",
    "        for tier_content_link in tier_content_links:\n",
    "            hash_id = hash_function(tier_content_link)\n",
    "            message = {'Id': hash_id, 'MessageGroupId': hash_id,\n",
    "                       'MessageBody': tier_content_link}\n",
    "\n",
    "#             thread_sqs = threading.Thread(target=self.send_to_sqs, args=(message,))\n",
    "#             threads.append(thread_sqs)\n",
    "#             thread_sqs.start()\n",
    "\n",
    "#             # self.send_to_sqs(message)\n",
    "\n",
    "#             if export_to_s3 is not None and export_to_s3 == \"On\":\n",
    "#                 message_body = {'hash_id': hash_id, 'page_url': tier_content_link}\n",
    "#                 self.send_to_s3(message_body, hash_id)\n",
    "#                 thread_s3 = threading.Thread(target=self.send_to_sqs, args=(message,))\n",
    "#                 threads.append(thread_s3)\n",
    "#                 thread_s3.start()\n",
    "\n",
    "#     @staticmethod\n",
    "#     def send_to_sqs(message):\n",
    "#         # Send the message to SQS\n",
    "#         try:\n",
    "#             # print(message['MessageGroupId'])\n",
    "#             response = sqs.send_message(QueueUrl=queue_tier_content_links,\n",
    "#                                         MessageGroupId=message['MessageGroupId'],\n",
    "#                                         MessageBody=message['MessageBody'],\n",
    "#                                         MessageDeduplicationId=message['MessageGroupId'])\n",
    "#             if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n",
    "#                 print('Fail')\n",
    "#         except Exception as e:\n",
    "#             print(str(e))\n",
    "\n",
    "#     @staticmethod\n",
    "#     def send_to_s3(m, hash_id):\n",
    "#         # Send the message to S3\n",
    "#         try:\n",
    "#             json_file_name = f'{hash_id}.json'\n",
    "#             response = s3.put_object(Bucket=s3_tier_content_links, Key=json_file_name,\n",
    "#                                      Body=json.dumps(m, ensure_ascii=False))\n",
    "#             # check if it's successful\n",
    "#             if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n",
    "#                 print('Fail')\n",
    "#         except Exception as e:\n",
    "#             print(str(e))\n",
    "\n",
    "    def spider_closed(self):\n",
    "        # Handle the spider closing event\n",
    "        self.runner.timeout = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ec3f74c-1e1e-4c6c-998b-01b6a556710b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LambdaRunner:\n",
    "    target_url = \"\"\n",
    "    receipt_handle = \"\"\n",
    "    tier4_content_object = None\n",
    "    timeout = False\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.finished = threading.Event()\n",
    "        self.results = []\n",
    "        if url != \"\":\n",
    "            self.target_url = url\n",
    "\n",
    "    def run_spider(self):\n",
    "        # Create a CrawlerRunner with project settings\n",
    "        settings = get_project_settings()\n",
    "        runner = CrawlerRunner(settings)\n",
    "        if self.target_url == \"\":\n",
    "            self.get_tier4_content_url_from_sqs()\n",
    "\n",
    "        # Callback function to handle the spider results\n",
    "        def handle_results(result):\n",
    "            self.results.append(result)\n",
    "\n",
    "            # Check if the spider has finished running\n",
    "            if len(self.results) == 1:\n",
    "                self.finished.set()\n",
    "\n",
    "        # Start the first spider run\n",
    "        deferred = runner.crawl(IOpenMallSpider, self) #modify\n",
    "        deferred.addCallback(handle_results)\n",
    "\n",
    "        # Start the reactor\n",
    "        runner.join()\n",
    "\n",
    "    def wait_for_completion(self):\n",
    "        self.finished.wait()\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "\n",
    "    def get_tier4_content_url_from_sqs(self):\n",
    "        response = sqs.receive_message(\n",
    "            QueueUrl=queue_tier_content_links,\n",
    "            MaxNumberOfMessages=1,  # Retrieve 10 messages\n",
    "            WaitTimeSeconds=0  # Maximum time to wait for messages (long polling)\n",
    "        )\n",
    "        messages = response.get('Messages', [])\n",
    "        if messages is not None:\n",
    "            self.tier4_content_object = messages[0]\n",
    "        else:\n",
    "            msg = \"All consumed.\"\n",
    "            print(msg)\n",
    "            return msg\n",
    "\n",
    "        # Delete messages from SQS\n",
    "        for message in messages:\n",
    "            sqs.delete_message(\n",
    "                QueueUrl=queue_tier_content_links,\n",
    "                ReceiptHandle=message['ReceiptHandle']\n",
    "            )\n",
    "\n",
    "\n",
    "def handler(event, context):\n",
    "    try:\n",
    "        # Check if the function was triggered by an HTTP request or Lambda event\n",
    "        print(f\"Starting to crawl:{datetime.datetime.now()}\")\n",
    "        times = 0\n",
    "        if \"statusCode\" not in event:\n",
    "            # If the function was not triggered by retry\n",
    "            runner = LambdaRunner(\"\")\n",
    "            runner.tier1 = event[\"tier\"][\"tier1\"]    # add on 08/30\n",
    "            runner.tier2 = event[\"tier\"][\"tier2\"]    # add on 08/30 \n",
    "            runner.tier3 = event[\"tier\"][\"tier3\"]    # add on 08/30\n",
    "            runner.tier4 = event[\"tier\"][\"tier4\"] if event[\"tier\"][\"category_type\"] == 'tier4' else None   # add on 08/30\n",
    "            runner.tier5 = event[\"tier\"][\"tier5\"] if event[\"tier\"][\"category_type\"] == 'tier5' else None   # add on 08/30\n",
    "            runner.category_type = event[\"tier\"][\"category_type\"]  # add on 08/30\n",
    "            runner.run_spider()\n",
    "            runner.wait_for_completion()\n",
    "            print(f\"End date and time: {datetime.datetime.now()}\")\n",
    "        else:\n",
    "            times = int(event[\"times\"])\n",
    "            if times < 4:\n",
    "                runner = LambdaRunner(event[\"tier_content_link\"])\n",
    "                runner.input_url = event[\"tier_content_link\"]\n",
    "                runner.tier1 = event[\"tier\"][\"tier1\"]    # add on 08/30\n",
    "                runner.tier2 = event[\"tier\"][\"tier2\"]    # add on 08/30\n",
    "                runner.tier3 = event[\"tier\"][\"tier3\"]    # add on 08/30\n",
    "                runner.tier4 = event[\"tier\"][\"tier4\"] if event[\"tier\"][\"category_type\"] == 'tier4' else None   # add on 08/30\n",
    "                runner.tier5 = event[\"tier\"][\"tier5\"] if event[\"tier\"][\"category_type\"] == 'tier5' else None   # add on 08/30\n",
    "                runner.category_type = event[\"tier\"][\"category_type\"]  # add on 08/30\n",
    "                runner.run_spider()\n",
    "                runner.wait_for_completion()\n",
    "                print(f\"End date and time: {datetime.datetime.now()}\")\n",
    "            else:\n",
    "                print(f'Retry too many times, 429: {event[\"tier_content_link\"]}')\n",
    "                # If the retry count is 4 or more, return an HTTP 429 response indicating Too Many Requests\n",
    "                return {\n",
    "                    'statusCode': 429,\n",
    "                    'body': \"\",\n",
    "                    'times': times,\n",
    "                    \"category_link\": event[\"tier_content_link\"]\n",
    "                }\n",
    "\n",
    "        times = times + 1\n",
    "        if not runner.timeout:\n",
    "            print(\"success\")\n",
    "            # If the LambdaRunner completed successfully, return an HTTP 200 response with the completion details\n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': 'Completed!',\n",
    "                'times': times\n",
    "            }\n",
    "        else:\n",
    "            print('timeout')\n",
    "            # If the LambdaRunner timed out, return an HTTP 408 response with the category objects\n",
    "            return {\n",
    "                'statusCode': 408,\n",
    "                'times': times,\n",
    "                \"category_link\": json.loads(runner.tier4_content_object['Body'])['tier_content_link']\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f'fail:{e}')\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({'error': str(e)})\n",
    "        }\n",
    "\n",
    "# #\n",
    "# if __name__ == '__main__':\n",
    "#     handler(\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed63adf8-a031-429f-9044-fa4f85980663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to crawl:2023-08-30 07:58:23.372268\n",
      "Starting start_requests\n",
      "tier_content_link: https://mall.iopenmall.tw/016725/index.php?action=product_detail&prod_no=P1672501667466\n",
      "Starting to parse\n",
      "{'tier1': 'æµè¡Œæ™‚å°š', 'tier2': 'å¥³éž‹', 'tier3': 'è·Ÿéž‹', 'tier4': '', 'tier5': '', 'category_type': 'tier3'}\n",
      "{'item_brand': 'éŸ“ç‰ˆå¥³è£ï¼Žå¥³éž‹', 'item_no': 'P1672501667466', 'item_name': 'ã€leleshopã€‘36-40å°çš®éž‹é«˜è·Ÿæ¶¼æ‹–éž‹ç´ è‰²ç²—è·Ÿé«˜è·Ÿéž‹ç‘ªèŽ‰çéž‹æ‡¶äººéž‹ç‰›æ´¥éž‹æ–¹é ­é«˜è·Ÿéž‹#3286', 'item_note': 'ðŸŽ€ç²—è·Ÿå°çš®éž‹ #3268//ðŸ”¶é¡è‰²ï¼šé»‘è‰²ã€ç±³è‰²ã€è±†æ²™è‰²//ðŸ”¸åº•åŽšåº¦8å…¬åˆ†//ðŸ”¶å°ºå¯¸ï¼š//36ï¼ˆé©åˆè…³é•·23ï¼‰//37ï¼ˆé©åˆè…³é•·23.5ï¼‰//38ï¼ˆé©åˆè…³é•·24ï¼‰//39ï¼ˆé©åˆè…³é•·24.5ï¼‰//40ï¼ˆé©åˆè…³é•·25ï¼‰//ðŸŒ¸ç‰ˆåž‹ï¼šæ­£å¸¸ç‰ˆ//â­•ï¸è¶…ç”œåƒ¹$550â¤ï¸', 'now_price': 550, 'past_price': '', 'activity': ['0829-0830-æ»¿$399ç¾æŠ˜$40'], 'payway': 'è¶…å•†å–è²¨ä»˜æ¬¾', 'deliveryway': 'å¸¸æº«7-ELEVENåº—åˆ°åº—å–è²¨ä»˜æ¬¾', 'item_feature': ' ç²—è·Ÿå°çš®éž‹ 3268 æ¯å€‹é¡è‰²éƒ½è¶…ç¾Žçš„å•¦ ç‰ˆåª½å»ºè­°å¯ä»¥ç›´æŽ¥åŒ…è‰²è¼ªæµæ›¿æ› é¡è‰² é»‘è‰² ç±³è‰² è±†æ²™è‰² åº•åŽšåº¦8å…¬åˆ† å°ºå¯¸ 36 é©åˆè…³é•·23 37 é©åˆè…³é•·23 5 38 é©åˆè…³é•·24 39 é©åˆè…³é•·24 5 40 é©åˆè…³é•·25 ç‰ˆåž‹ æ­£å¸¸ç‰ˆ è¶…ç”œåƒ¹ 550 æ¯å°é¡¯ç¤ºå™¨çš†ä¸åŒ ä¾å¯¦å“é¡è‰²ç‚ºä¸» é«˜æ¨™å®Œç¾Žä¸»ç¾©è€…è«‹ç¹žé“è€Œè¡Œ ä¸‹å–®å‰è«‹æ³¨æ„ ç¾è²¨ é è³¼ è¨­å®š7å¤©å‡ºè²¨åªæ˜¯æ“”å¿ƒè¦çš® 3å¤©å¾Œè‡ªå‹•å–æ¶ˆè¨‚å–® å•†å“åŸºæœ¬ä¸Š2 3å¤©å°±å¯ä»¥æ”¶åˆ° å¦‚æžœéœ€è¦è¨‚è³¼éž‹å­è«‹ç­‰ 3 15å¤© æ€•ç­‰å¾…çš„æ°´æ°´å¯å…ˆè©¢å•åœ¨ä¸‹å–® å°æˆ‘å€‘çš„å•†å“å’Œæœå‹™æ»¿æ„çš„ éº»ç…©æ‚¨çµ¦äº”æ˜Ÿå¥½è©•å“¦ æœ‰ä¸æ»¿æ„çš„åœ°æ–¹éº»ç…©æ‚¨åœ¨èŠèŠä¸Šé¢è¯ç¹«æˆ‘å€‘ ', 'store_seller': [{'å•†å“': '87'}, {'åŠ å…¥æ™‚é–“': '2023-07-07'}, {'è©•åƒ¹': '5.0/5.0'}, {'è³¼è²·äººæ¬¡': '1äºº'}], 'goods_commend': {'count': 0}, 'goods_commend_review_card_list': [], 'data_date': '20230830'}\n",
      "End date and time: 2023-08-30 07:58:26.449031\n",
      "success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200, 'body': 'Completed!', 'times': 2}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler({'statusCode': 200,'times': 1, \n",
    "    \"tier_content_link\": \"https://mall.iopenmall.tw/016725/index.php?action=product_detail&prod_no=P1672501667466\", \n",
    "    \"tier\": {\"tier1\": \"æµè¡Œæ™‚å°š\", \"tier2\": \"å¥³éž‹\", \"tier3\": \"è·Ÿéž‹\", \"tier4\": '', \"tier5\": '', \"category_type\": \"tier3\"}}, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b439ba-6d6a-4366-a22a-a15b4ab81154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4faf03-682d-455a-b17b-585fd78dc124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
