{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bd6f9b0-3799-4bc7-bc69-1795f35f1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = utf-8\n",
    "import pandas as pd\n",
    "import json, sys, time, re,heapq, io, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchtext.data import utils\n",
    "from torchtext.data.utils import _basic_english_normalize\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3635234-f4eb-4653-9c30-decb6e10d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(f'Model/opei_classifier_v4.pt')\n",
    "model.eval()\n",
    "class_dict = json.loads(io.open(f'Model/dict_class_v4.json', encoding='utf-8').read())\n",
    "\n",
    "class_idx_to_name = {}\n",
    "for k in class_dict.keys():\n",
    "\tclass_idx_to_name[class_dict[k]] = k\n",
    "    \n",
    "stoi = json.loads(io.open(f'Model/stoi-v4.json', encoding='utf-8').read())\n",
    "\n",
    "df_class = pd.read_csv('Model/dict_class_comparison.csv')\n",
    "\n",
    "\n",
    "cmd = '''\n",
    "awk '{print $1}' Classify_setting.txt\n",
    "'''%locals()\n",
    "settingfile=os.popen(cmd).readlines()\n",
    "inputfile = settingfile[1].rsplit('\\n')[0]\n",
    "outputfile = settingfile[3].rsplit('\\n')[0]\n",
    "dtest = pd.read_csv(inputfile)\n",
    "Num_keyword = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be76dd3c-5b99-45b6-ac99-8f72d62f4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "def CJK_cleaner(string): \n",
    "    filters = re.compile(u'[^0-9a-zA-Z\\u4e00-\\u9fff]+', re.UNICODE)\n",
    "    return filters.sub('', string)    \n",
    "    \n",
    "def classify(text):\n",
    "    text = CJK_cleaner(text)\n",
    "    text = re.sub(r'([\\u4e00-\\u9fff])',r' \\1',text)\n",
    "    l = list(utils.ngrams_iterator(_basic_english_normalize(text),2))\n",
    "    l = [[stoi.get(token, 0) for token in l]]\n",
    "\n",
    "    text = torch.tensor(l)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        result = model(text, None)\n",
    "        value, index = (torch.sort(result,descending=True))\n",
    "        Maxidx = index[0][0].item()+1\n",
    "        # print(\"===========================================\")\n",
    "        # for i in range(0,5):\n",
    "        #     classIdx = index[0][i].item()\n",
    "        #     if i == 0 : \n",
    "        #         Maxidx = classIdx+1\n",
    "        #     score = value[0][i].item()\n",
    "        #     print(f\"class: {class_idx_to_name[classIdx+1]}, score:{score}\")\n",
    "        # print(\"===========================================\")\n",
    "        return index[0][0].item()+1, class_idx_to_name[Maxidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f57dd01c-3be7-4e94-b261-3b5c8afe7c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特大杯熱咖啡密斯朵20230114h5fk3ubm\n",
      "大分類: 1, 咖啡類\n",
      "中分類: 1, 現做咖啡飲品\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m keywordnum \u001b[38;5;241m=\u001b[39m df_keyword[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserchkey_subname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_no\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     34\u001b[0m find_keyword \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 35\u001b[0m find_keyword \u001b[38;5;241m=\u001b[39m  [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m:keyname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m:keynum} \u001b[38;5;28;01mfor\u001b[39;00m keyname, keynum \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keywordname,keywordnum)  \u001b[38;5;28;01mif\u001b[39;00m keyname \u001b[38;5;129;01min\u001b[39;00m product ]\n\u001b[1;32m     36\u001b[0m find_keyword_topN \u001b[38;5;241m=\u001b[39m heapq\u001b[38;5;241m.\u001b[39mnlargest(Num_keyword, find_keyword, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m find_keyword: find_keyword[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     37\u001b[0m find_keyword_topN \u001b[38;5;241m=\u001b[39m [temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m find_keyword_topN]\n",
      "Cell \u001b[0;32mIn[21], line 35\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m keywordnum \u001b[38;5;241m=\u001b[39m df_keyword[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserchkey_subname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_no\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     34\u001b[0m find_keyword \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 35\u001b[0m find_keyword \u001b[38;5;241m=\u001b[39m  [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m:keyname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m:keynum} \u001b[38;5;28;01mfor\u001b[39;00m keyname, keynum \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keywordname,keywordnum)  \u001b[38;5;28;01mif\u001b[39;00m keyname \u001b[38;5;129;01min\u001b[39;00m product ]\n\u001b[1;32m     36\u001b[0m find_keyword_topN \u001b[38;5;241m=\u001b[39m heapq\u001b[38;5;241m.\u001b[39mnlargest(Num_keyword, find_keyword, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m find_keyword: find_keyword[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     37\u001b[0m find_keyword_topN \u001b[38;5;241m=\u001b[39m [temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m find_keyword_topN]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not float"
     ]
    }
   ],
   "source": [
    "tempdf = {'name':[], \n",
    "        'category_no':[],\n",
    "        'category_name':[],\n",
    "        'sub_no':[],\n",
    "        'sub_name': [] } \n",
    "\n",
    "for num in range(5):\n",
    "    locals()[f'keyword_{num+1}']=[]\n",
    "for p_idx in range(len(dtest)):\n",
    "    product = dtest['product'].values[p_idx]\n",
    "    \n",
    "#================== using nn model to find subcategory\n",
    "\n",
    "\n",
    "    sub_no,sub_name = classify(product)\n",
    "\n",
    "    print(product)\n",
    "    \n",
    "\n",
    "    #================== subcategory to category\n",
    "    categoryname = df_class['大分類'][df_class['中分類編號']==sub_no].values[0]\n",
    "    category_no  = df_class['大分類編號'][df_class['中分類編號']==sub_no].values[0]\n",
    "\n",
    "    print(f'大分類: {category_no}, {categoryname}')\n",
    "    print(f'中分類: {sub_no}, {sub_name}')\n",
    "    #================== using product name to find keyword\n",
    "    # try:\n",
    "    df_keyword = pd.read_csv(f'Model/Keyword_clear.csv')\n",
    "    \n",
    "    serchkey_subname = sub_name.replace('/','_')\n",
    "    keywordname = df_keyword[f'{serchkey_subname}_key'].tolist()\n",
    "    keywordnum = df_keyword[f'{serchkey_subname}_no'].tolist()\n",
    "\n",
    "    find_keyword = []\n",
    "    find_keyword = [{'name':keyname, 'num':keynum} for keyname, keynum in zip(keywordname,keywordnum)  if keyname in product ]\n",
    "    find_keyword_topN = heapq.nlargest(Num_keyword, find_keyword, key=lambda find_keyword: find_keyword['num'])\n",
    "    find_keyword_topN = [temp['name'] for temp in find_keyword_topN]\n",
    "    print(f'標籤: {find_keyword_topN}')\n",
    "    print()\n",
    "    print()\n",
    "    # except:\n",
    "        # find_keyword = []\n",
    "    #================== clean up information to csv\n",
    "\n",
    "    tempdf['name'].append(product)\n",
    "    tempdf['category_no'].append(category_no)\n",
    "    tempdf['category_name'].append(categoryname)     \n",
    "    tempdf['sub_no'].append(sub_no)\n",
    "    tempdf['sub_name'].append(sub_name)   \n",
    "\n",
    "    for idx, keyword in enumerate(find_keyword_topN):\n",
    "        if idx <=4:\n",
    "            locals()[f'keyword_{idx+1}'].append(keyword)\n",
    "\n",
    "    if len(find_keyword_topN) < Num_keyword:\n",
    "        for nanidx in range(len(find_keyword_topN),Num_keyword):\n",
    "            locals()[f'keyword_{nanidx+1}'].append(np.nan)\n",
    "\n",
    "\n",
    "tempdf = pd.DataFrame(tempdf)\n",
    "\n",
    "keytempdf = pd.DataFrame({'keyword_1': keyword_1, \n",
    "                          'keyword_2': keyword_2, \n",
    "                          'keyword_3': keyword_3, \n",
    "                          'keyword_4': keyword_4, \n",
    "                          'keyword_5': keyword_5 })\n",
    "\n",
    "newdf = pd.concat([tempdf,keytempdf], axis=1)\n",
    "\n",
    "\n",
    "# newdf.to_csv(outputfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6ab9b3ea-c547-46dd-80c3-5c82e3589818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cc = os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c7ec290-1c28-4be8-947a-139574f03f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=os.popen(cmd)\n",
    "\n",
    "settingfile = f.readlines()\n",
    "\n",
    "\n",
    "settingfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68eb3e82-6303-4830-ba8b-4da7ecc112c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!open .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c221b-8ae7-45f9-974a-25c2f6b11328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
